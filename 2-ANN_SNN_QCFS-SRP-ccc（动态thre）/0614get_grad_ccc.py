#!/usr/bin/env python3

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import argparse
import os
import sys
from datetime import datetime
from Models import modelpool
from Preprocess import datapool
from utils import seed_all
import pandas as pd
import warnings
from Models.layer import IF
# from Models.layer import load_model_compatible

# è®¾ç½®ç¯å¢ƒå˜é‡æŠ‘åˆ¶cuDNNè­¦å‘Š
os.environ['CUDNN_V8_API_DISABLED'] = '1'
warnings.filterwarnings("ignore", category=UserWarning)
# æŠ‘åˆ¶PyTorchç›¸å…³è­¦å‘Š
import torch.backends.cudnn as cudnn
cudnn.benchmark = True
torch.backends.cudnn.deterministic = False


class ComprehensiveNeuronAnalyzer:
    """ç¥ç»å…ƒæ¢¯åº¦åˆ†æå™¨"""
    def __init__(self, model):
        self.model = model
        self.weight_grad_hooks = {}
        self.tensor_grad_hooks = {}
        self.if_grad_hooks = {}
        self.gradient_records = {}
        
    def register_comprehensive_hooks(self):
        """æ³¨å†Œæ¢¯åº¦é’©å­"""
        # ç§»é™¤ç°æœ‰é’©å­
        for handle in self.weight_grad_hooks.values():
            handle.remove()
        for handle in self.tensor_grad_hooks.values():
            handle.remove()
        for handle in self.if_grad_hooks.values():
            handle.remove()
        self.weight_grad_hooks = {}
        self.tensor_grad_hooks = {}
        self.if_grad_hooks = {}
        self.gradient_records = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰å…¨è¿æ¥å±‚å’Œç›®æ ‡IFå±‚
        fc_count = 0
        if_count = 0
        target_if_layers = ['classifier.2', 'classifier.5']
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                fc_count += 1
                
                # 1. æƒé‡æ¢¯åº¦é’©å­
                weight_hook = self._weight_gradient_hook(name)
                weight_handle = module.weight.register_hook(weight_hook)
                self.weight_grad_hooks[name] = weight_handle
                
                # 2. å¼ é‡æ¢¯åº¦é’©å­ï¼ˆgrad_inputå’Œgrad_outputï¼‰
                tensor_hook = self._tensor_gradient_hook(name)
                tensor_handle = module.register_full_backward_hook(tensor_hook)
                self.tensor_grad_hooks[name] = tensor_handle
                
                # åˆå§‹åŒ–è®°å½•
                self.gradient_records[name] = {
                    'layer_type': 'fc',
                    'weight_grad': None,      # æƒé‡æ¢¯åº¦
                    'input_grad': None,       # è¾“å…¥æ¢¯åº¦
                    'output_grad': None,      # è¾“å‡ºæ¢¯åº¦
                    'importance_scores': {}   # ç»¼åˆé‡è¦æ€§åˆ†æ•°
                }
            
            # æ·»åŠ IFå±‚æ¢¯åº¦é‡‡é›† (ä»…é’ˆå¯¹classifier.2å’Œclassifier.5)
            elif isinstance(module, IF) and name in target_if_layers:
                if_count += 1
                
                # 1. é˜ˆå€¼æ¢¯åº¦é’©å­
                thresh_hook = self._threshold_gradient_hook(name)
                thresh_handle = module.thresh.register_hook(thresh_hook)
                self.if_grad_hooks[f"{name}_thresh"] = thresh_handle
                
                # 2. IFå±‚å¼ é‡æ¢¯åº¦é’©å­
                if_tensor_hook = self._if_tensor_gradient_hook(name)
                if_tensor_handle = module.register_full_backward_hook(if_tensor_hook)
                self.if_grad_hooks[f"{name}_tensor"] = if_tensor_handle
                
                # åˆå§‹åŒ–IFå±‚è®°å½•
                self.gradient_records[name] = {
                    'layer_type': 'if',
                    'threshold_grad': None,   # é˜ˆå€¼æ¢¯åº¦
                    'input_grad': None,       # è¾“å…¥æ¢¯åº¦
                    'output_grad': None,      # è¾“å‡ºæ¢¯åº¦
                    'threshold_value': None,  # é˜ˆå€¼æ•°å€¼
                }
        
        print(f"æ€»å…±æ³¨å†Œäº† {fc_count} ä¸ªå…¨è¿æ¥å±‚å’Œ {if_count} ä¸ªIFå±‚çš„æ¢¯åº¦é’©å­")
        
    def _weight_gradient_hook(self, name):
        """æƒé‡æ¢¯åº¦é’©å­"""
        def hook(grad):
            if grad is not None:
                # è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å¹³å‡æƒé‡æ¢¯åº¦
                if grad.dim() > 1:
                    neuron_weight_grads = grad.abs().mean(dim=1)  # [out_features]
                else:
                    neuron_weight_grads = grad.abs()
                self.gradient_records[name]['weight_grad'] = neuron_weight_grads.detach().cpu()
        return hook
    
    def _tensor_gradient_hook(self, name):
        """å¼ é‡æ¢¯åº¦é’©å­"""
        def hook(module, grad_input, grad_output):
            # æ•è·è¾“å…¥æ¢¯åº¦
            if grad_input[0] is not None:
                input_grad = grad_input[0]  # [batch_size, in_features]
                # è®¡ç®—æ¯ä¸ªè¾“å…¥ç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
                neuron_input_grads = input_grad.abs().mean(dim=0)  # [in_features]
                self.gradient_records[name]['input_grad'] = neuron_input_grads.detach().cpu()
            
            # æ•è·è¾“å‡ºæ¢¯åº¦
            if grad_output[0] is not None:
                output_grad = grad_output[0]  # [batch_size, out_features]
                # è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
                neuron_output_grads = output_grad.abs().mean(dim=0)  # [out_features]
                self.gradient_records[name]['output_grad'] = neuron_output_grads.detach().cpu()
        return hook
    
    def _threshold_gradient_hook(self, name):
        """IFå±‚é˜ˆå€¼æ¢¯åº¦é’©å­"""
        def hook(grad):
            if grad is not None:
                # é˜ˆå€¼æ¢¯åº¦æ˜¯æ ‡é‡
                thresh_grad = grad.abs().item()
                self.gradient_records[name]['threshold_grad'] = thresh_grad
        return hook
    
    def _if_tensor_gradient_hook(self, name):
        """IFå±‚å¼ é‡æ¢¯åº¦é’©å­"""
        def hook(module, grad_input, grad_output):
            # ä¿å­˜é˜ˆå€¼æ•°å€¼
            self.gradient_records[name]['threshold_value'] = module.thresh.data.item()
            
            # æ•è·è¾“å…¥æ¢¯åº¦
            if grad_input[0] is not None:
                input_grad = grad_input[0]  # [batch_size, features] or [T*batch_size, features]
                # å¤„ç†SNNæ¨¡å¼çš„æ—¶é—´ç»´åº¦
                if module.T > 0:
                    # SNNæ¨¡å¼: reshapeå› [T, batch_size, features] ç„¶ååœ¨æ—¶é—´ç»´åº¦å¹³å‡
                    batch_size = input_grad.shape[0] // module.T
                    input_grad = input_grad.view(module.T, batch_size, -1)
                    input_grad = input_grad.mean(dim=0)  # æ—¶é—´ç»´åº¦å¹³å‡: [batch_size, features]
                
                # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
                neuron_input_grads = input_grad.abs().mean(dim=0)  # [features]
                self.gradient_records[name]['input_grad'] = neuron_input_grads.detach().cpu()
            
            # æ•è·è¾“å‡ºæ¢¯åº¦
            if grad_output[0] is not None:
                output_grad = grad_output[0]  # [batch_size, features] or [T*batch_size, features]
                # å¤„ç†SNNæ¨¡å¼çš„æ—¶é—´ç»´åº¦
                if module.T > 0:
                    # SNNæ¨¡å¼: reshapeå› [T, batch_size, features] ç„¶ååœ¨æ—¶é—´ç»´åº¦å¹³å‡
                    batch_size = output_grad.shape[0] // module.T
                    output_grad = output_grad.view(module.T, batch_size, -1)
                    output_grad = output_grad.mean(dim=0)  # æ—¶é—´ç»´åº¦å¹³å‡: [batch_size, features]
                
                # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
                neuron_output_grads = output_grad.abs().mean(dim=0)  # [features]
                self.gradient_records[name]['output_grad'] = neuron_output_grads.detach().cpu()
        return hook
    

    
    def analyze_gradients(self, dataloader, criterion, num_batches=5):
        """
        åˆ†æå…¨è¿æ¥å±‚æ¢¯åº¦åˆ†å¸ƒ
        
        å‚æ•°:
        dataloader - æ•°æ®åŠ è½½å™¨
        criterion - æŸå¤±å‡½æ•°
        num_batches - åˆ†ææ‰¹æ¬¡æ•°
        
        è¿”å›:
        gradient_stats - æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«ä¸‰ç§æ¢¯åº¦çš„å¹³å‡å€¼
        """
        print(f"\nå¼€å§‹åˆ†æ {num_batches} ä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦åˆ†å¸ƒ...")
        
        # æ³¨å†Œæ¢¯åº¦é’©å­
        self.register_comprehensive_hooks()
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        self.model.train()
        
        # æ¢¯åº¦ç»Ÿè®¡æ”¶é›†å™¨
        gradient_stats = {}
        for name in self.gradient_records.keys():
            layer_type = self.gradient_records[name]['layer_type']
            if layer_type == 'fc':
                gradient_stats[name] = {
                    'layer_type': 'fc',
                    'weight_grad_values': [],
                    'input_grad_values': [],
                    'output_grad_values': []
                }
            elif layer_type == 'if':
                gradient_stats[name] = {
                    'layer_type': 'if',
                    'threshold_grad_values': [],
                    'input_grad_values': [],
                    'output_grad_values': [],
                    'threshold_values': []
                }
        
        # å¤„ç†æŒ‡å®šæ‰¹æ¬¡æ•°æ®
        batch_count = 0
        data_iter = iter(dataloader)
        
        for batch_idx in range(num_batches):
            try:
                inputs, targets = next(data_iter)
                inputs, targets = inputs.to(next(self.model.parameters()).device), targets.to(next(self.model.parameters()).device)
            except StopIteration:
                print(f"æ•°æ®ä¸è¶³ï¼Œåªå¤„ç†äº† {batch_idx} ä¸ªæ‰¹æ¬¡")
                break
                
            # æ¸…ç©ºæ¢¯åº¦
            self.model.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(inputs)
            
            # å¤„ç†SNNè¾“å‡º
            if len(outputs.shape) > 2:
                outputs = outputs.mean(0)  # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, targets)
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            total = targets.size(0)
            correct = (predicted == targets).sum().item()
            accuracy = 100 * correct / total
            
            # åå‘ä¼ æ’­ï¼ˆè§¦å‘æ¢¯åº¦é’©å­ï¼‰
            loss.backward()
            
            # æ”¶é›†æ¢¯åº¦æ•°æ®
            for name, records in self.gradient_records.items():
                layer_type = records['layer_type']
                
                if layer_type == 'fc':
                    # FCå±‚æ¢¯åº¦æ”¶é›†
                    if records['weight_grad'] is not None:
                        gradient_stats[name]['weight_grad_values'].append(records['weight_grad'].numpy())
                    if records['input_grad'] is not None:
                        gradient_stats[name]['input_grad_values'].append(records['input_grad'].numpy())
                    if records['output_grad'] is not None:
                        gradient_stats[name]['output_grad_values'].append(records['output_grad'].numpy())
                
                elif layer_type == 'if':
                    # IFå±‚æ¢¯åº¦æ”¶é›†
                    if records['threshold_grad'] is not None:
                        gradient_stats[name]['threshold_grad_values'].append(records['threshold_grad'])
                    if records['input_grad'] is not None:
                        gradient_stats[name]['input_grad_values'].append(records['input_grad'].numpy())
                    if records['output_grad'] is not None:
                        gradient_stats[name]['output_grad_values'].append(records['output_grad'].numpy())
                    if records['threshold_value'] is not None:
                        gradient_stats[name]['threshold_values'].append(records['threshold_value'])
            
            batch_count += 1
            print(f"  å¤„ç†æ‰¹æ¬¡ {batch_count}/{num_batches}, æŸå¤±: {loss.item():.6f}, å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦
        for name in gradient_stats:
            layer_type = gradient_stats[name]['layer_type']
            
            if layer_type == 'fc':
                # FCå±‚å¹³å‡æ¢¯åº¦è®¡ç®—
                if gradient_stats[name]['weight_grad_values']:
                    gradient_stats[name]['weight_grad_values'] = np.mean(gradient_stats[name]['weight_grad_values'], axis=0)
                if gradient_stats[name]['input_grad_values']:
                    gradient_stats[name]['input_grad_values'] = np.mean(gradient_stats[name]['input_grad_values'], axis=0)
                if gradient_stats[name]['output_grad_values']:
                    gradient_stats[name]['output_grad_values'] = np.mean(gradient_stats[name]['output_grad_values'], axis=0)
            
            elif layer_type == 'if':
                # IFå±‚å¹³å‡æ¢¯åº¦è®¡ç®—
                if gradient_stats[name]['threshold_grad_values']:
                    gradient_stats[name]['threshold_grad_values'] = np.mean(gradient_stats[name]['threshold_grad_values'])
                if gradient_stats[name]['input_grad_values']:
                    gradient_stats[name]['input_grad_values'] = np.mean(gradient_stats[name]['input_grad_values'], axis=0)
                if gradient_stats[name]['output_grad_values']:
                    gradient_stats[name]['output_grad_values'] = np.mean(gradient_stats[name]['output_grad_values'], axis=0)
                if gradient_stats[name]['threshold_values']:
                    gradient_stats[name]['threshold_values'] = np.mean(gradient_stats[name]['threshold_values'])
        
        return gradient_stats

    def analyze_gradient_correlation(self, gradient_stats):
        """åˆ†æä¸åŒæ¢¯åº¦ç±»å‹ä¹‹é—´çš„ç›¸å…³æ€§"""
        print("\n" + "="*80)
        print("æ¢¯åº¦ç›¸å…³æ€§åˆ†æ")
        print("="*80)
        
        try:
            import scipy.stats
        except ImportError:
            print("éœ€è¦å®‰è£…scipyåº“: pip install scipy")
            return
        
        # FCå±‚ä¸IFå±‚çš„å¯¹åº”å…³ç³»æ˜ å°„
        fc_to_if_mapping = {
            'classifier.1': 'classifier.2',
            'classifier.4': 'classifier.5',
        }
        
        for layer_name, stats in gradient_stats.items():
            # åªåˆ†æFCå±‚
            if stats.get('layer_type') != 'fc':
                continue
                
            print(f"\nå±‚: {layer_name}")
            
            weight_grad = stats.get('weight_grad_values')
            output_grad = stats.get('output_grad_values') 
            input_grad = stats.get('input_grad_values')
            
            # è·å–å¯¹åº”IFå±‚çš„æ¢¯åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if_output_grad = None
            if layer_name in fc_to_if_mapping:
                if_layer_name = fc_to_if_mapping[layer_name]
                if if_layer_name in gradient_stats:
                    if_stats = gradient_stats[if_layer_name]
                    if_output_grad = if_stats.get('output_grad_values')
            
            # åˆ†æFCå±‚å†…éƒ¨æ¢¯åº¦çš„ç›¸å…³æ€§
            if weight_grad is not None and output_grad is not None:
                # è®¡ç®—æƒé‡æ¢¯åº¦å’Œè¾“å‡ºæ¢¯åº¦çš„ç›¸å…³ç³»æ•°
                corr_coef, p_value = scipy.stats.pearsonr(weight_grad, output_grad)
                print(f"  æƒé‡æ¢¯åº¦ vs è¾“å‡ºæ¢¯åº¦:")
                print(f"    çš®å°”é€Šç›¸å…³ç³»æ•°: {corr_coef:.6f}")
                print(f"    På€¼: {p_value:.2e}")
                
                # è®¡ç®—æ’åºç›¸å…³æ€§ï¼ˆè¿™å¯¹å‰ªææ›´é‡è¦ï¼‰
                from scipy.stats import spearmanr
                rank_corr, rank_p = spearmanr(weight_grad, output_grad)
                print(f"    æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°: {rank_corr:.6f}")
                print(f"    På€¼: {rank_p:.2e}")
                
                # åˆ†ææ¢¯åº¦æ¯”å€¼çš„åˆ†å¸ƒ
                if np.all(output_grad > 1e-10):  # é¿å…é™¤é›¶
                    ratio = weight_grad / output_grad
                    print(f"    æƒé‡æ¢¯åº¦/è¾“å‡ºæ¢¯åº¦ æ¯”å€¼ç»Ÿè®¡:")
                    print(f"      å‡å€¼: {ratio.mean():.6f}")
                    print(f"      æ ‡å‡†å·®: {ratio.std():.6f}")
                    print(f"      å˜å¼‚ç³»æ•°: {ratio.std()/ratio.mean():.6f}")
            
            # åˆ†æFCå±‚ä¸å¯¹åº”IFå±‚æ¢¯åº¦çš„ç›¸å…³æ€§
            if if_output_grad is not None:
                if_layer_name = fc_to_if_mapping[layer_name]
                print(f"\n  FCå±‚ vs å¯¹åº”IFå±‚({if_layer_name})æ¢¯åº¦ç›¸å…³æ€§:")
                
                # FCå±‚æƒé‡æ¢¯åº¦ vs IFå±‚è¾“å‡ºæ¢¯åº¦
                if weight_grad is not None:
                    if len(weight_grad) == len(if_output_grad):
                        corr_coef, p_value = scipy.stats.pearsonr(weight_grad, if_output_grad)
                        rank_corr, rank_p = spearmanr(weight_grad, if_output_grad)
                        print(f"    FCæƒé‡æ¢¯åº¦ vs IFè¾“å‡ºæ¢¯åº¦:")
                        print(f"      çš®å°”é€Šç›¸å…³ç³»æ•°: {corr_coef:.6f} (På€¼: {p_value:.2e})")
                        print(f"      æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°: {rank_corr:.6f} (På€¼: {rank_p:.2e})")
                
                # FCå±‚è¾“å‡ºæ¢¯åº¦ vs IFå±‚è¾“å‡ºæ¢¯åº¦
                if output_grad is not None:
                    if len(output_grad) == len(if_output_grad):
                        corr_coef, p_value = scipy.stats.pearsonr(output_grad, if_output_grad)
                        rank_corr, rank_p = spearmanr(output_grad, if_output_grad)
                        print(f"    FCè¾“å‡ºæ¢¯åº¦ vs IFè¾“å‡ºæ¢¯åº¦:")
                        print(f"      çš®å°”é€Šç›¸å…³ç³»æ•°: {corr_coef:.6f} (På€¼: {p_value:.2e})")
                        print(f"      æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°: {rank_corr:.6f} (På€¼: {rank_p:.2e})")
            
            # åˆ†æå‰ªæç¥ç»å…ƒçš„é‡å åº¦
            if weight_grad is not None and output_grad is not None:
                print(f"\n  FCå±‚å†…éƒ¨æ¢¯åº¦å‰ªæé‡å åº¦:")
                # è®¡ç®—æœ€ä½ç¥ç»å…ƒçš„é‡å åº¦
                for ratio in [0.1, 0.2, 0.3]:
                    num_prune = int(len(weight_grad) * ratio)
                    
                    weight_indices = set(np.argsort(weight_grad)[:num_prune])
                    output_indices = set(np.argsort(output_grad)[:num_prune])
                    
                    overlap = len(weight_indices & output_indices)
                    overlap_ratio = overlap / num_prune if num_prune > 0 else 0
                    
                    print(f"    æœ€ä½{ratio*100:.0f}%ç¥ç»å…ƒé‡å åº¦: {overlap}/{num_prune} ({overlap_ratio:.2%})")
            
            # åˆ†æFCå±‚æ¢¯åº¦ä¸IFå±‚æ¢¯åº¦çš„å‰ªæé‡å åº¦
            if if_output_grad is not None and weight_grad is not None:
                if len(weight_grad) == len(if_output_grad):
                    print(f"\n  FCå±‚ vs IFå±‚å‰ªæé‡å åº¦:")
                    for ratio in [0.1, 0.2, 0.3]:
                        num_prune = int(len(weight_grad) * ratio)
                        
                        fc_weight_indices = set(np.argsort(weight_grad)[:num_prune])
                        if_output_indices = set(np.argsort(if_output_grad)[:num_prune])
                        
                        overlap = len(fc_weight_indices & if_output_indices)
                        overlap_ratio = overlap / num_prune if num_prune > 0 else 0
                        
                        print(f"    FCæƒé‡æ¢¯åº¦ vs IFè¾“å‡ºæ¢¯åº¦ æœ€ä½{ratio*100:.0f}%é‡å åº¦: {overlap}/{num_prune} ({overlap_ratio:.2%})")
        
        print("="*80)

    def get_comprehensive_pruning_neurons(self, gradient_stats, ratio=0.1, method='weight_grad_values'):
        """
        åŸºäºæ¢¯åº¦é‡è¦æ€§è¿›è¡Œå‰ªæ
        
        å‚æ•°:
        gradient_stats - analyze_gradientsè¿”å›çš„ç»Ÿè®¡æ•°æ®
        ratio - è¦å‰ªæçš„ç¥ç»å…ƒæ¯”ä¾‹
        method - æ¢¯åº¦ç±»å‹: 'weight_grad_values', 'input_grad_values', 'output_grad_values', 'IF_output_grad_values'
        
        è¿”å›:
        neurons_to_prune - è¦å‰ªæçš„ç¥ç»å…ƒåˆ—è¡¨
        """
        neurons_to_prune = []
        
        # FCå±‚ä¸IFå±‚çš„å¯¹åº”å…³ç³»æ˜ å°„
        fc_to_if_mapping = {
            'classifier.1': 'classifier.2',  # FCå±‚ -> å¯¹åº”çš„IFå±‚
            'classifier.4': 'classifier.5',  # FCå±‚ -> å¯¹åº”çš„IFå±‚
            # classifier.6 æ˜¯æœ€åä¸€å±‚ï¼Œä¸è¿›è¡Œå‰ªæ
        }
        
        for layer_name, stats in gradient_stats.items():
            if layer_name == 'classifier.7':  # è·³è¿‡æœ€åä¸€å±‚
                continue
                
            # åªå¤„ç†FCå±‚ï¼ˆè·³è¿‡IFå±‚ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ç›´æ¥å¯¹IFå±‚å‰ªæï¼‰
            if stats.get('layer_type') != 'fc':
                continue
            
            # æ ¹æ®methodé€‰æ‹©æ¢¯åº¦ç±»å‹
            if method == 'IF_output_grad_values':
                # ä½¿ç”¨å¯¹åº”IFå±‚çš„outputæ¢¯åº¦
                if layer_name not in fc_to_if_mapping:
                    print(f"è­¦å‘Š: FCå±‚ {layer_name} æ²¡æœ‰å¯¹åº”çš„IFå±‚ï¼Œè·³è¿‡")
                    continue
                
                if_layer_name = fc_to_if_mapping[layer_name]
                if if_layer_name not in gradient_stats:
                    print(f"è­¦å‘Š: å¯¹åº”çš„IFå±‚ {if_layer_name} æ²¡æœ‰æ¢¯åº¦æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                if_stats = gradient_stats[if_layer_name]
                if_output_grad = if_stats.get('output_grad_values')
                if_input_grad = if_stats.get('input_grad_values')
                
                if if_output_grad is None:
                    print(f"è­¦å‘Š: IFå±‚ {if_layer_name} æ²¡æœ‰ output_grad_values æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                if if_input_grad is None:
                    print(f"è­¦å‘Š: IFå±‚ {if_layer_name} æ²¡æœ‰ input_grad_values æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                # è·å–IFå±‚å‰ä¸€ä¸ªFCå±‚çš„æƒé‡å’Œæƒé‡æ¢¯åº¦
                fc_weight_grad = stats.get('weight_grad_values')
                if fc_weight_grad is None:
                    print(f"è­¦å‘Š: FCå±‚ {layer_name} æ²¡æœ‰ weight_grad_values æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                # è·å–FCå±‚çš„æƒé‡æ•°æ®
                fc_weight = None
                for name, module in self.model.named_modules():
                    if name == layer_name and isinstance(module, nn.Linear):
                        # è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å¹³å‡æƒé‡å€¼
                        fc_weight = module.weight.data.abs().mean(dim=1).cpu().numpy()
                        break
                
                if fc_weight is None:
                    print(f"è­¦å‘Š: æ‰¾ä¸åˆ°FCå±‚ {layer_name} çš„æƒé‡æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                # è®¡ç®—å››ä¸ªå€¼çš„åŠ å’Œï¼šIFå±‚è¾“å‡ºæ¢¯åº¦ + IFå±‚è¾“å…¥æ¢¯åº¦ + FCå±‚æƒé‡ + FCå±‚æƒé‡æ¢¯åº¦
                # grad_values = if_output_grad + if_input_grad + fc_weight + fc_weight_grad
                # grad_values = fc_weight / (if_input_grad/if_output_grad)
                grad_values = fc_weight / (if_output_grad/if_input_grad)
                # æ£€æŸ¥ç»´åº¦åŒ¹é…
                fc_output_size = None
                for name, module in self.model.named_modules():
                    if name == layer_name and isinstance(module, nn.Linear):
                        fc_output_size = module.out_features
                        break
                
                if fc_output_size is None:
                    print(f"è­¦å‘Š: æ‰¾ä¸åˆ°FCå±‚ {layer_name}ï¼Œè·³è¿‡")
                    continue
                
                if len(grad_values) != fc_output_size:
                    print(f"è­¦å‘Š: FCå±‚ {layer_name} è¾“å‡ºç»´åº¦({fc_output_size}) ä¸ IFå±‚ {if_layer_name} æ¢¯åº¦ç»´åº¦({len(grad_values)}) ä¸åŒ¹é…ï¼Œè·³è¿‡")
                    continue
                
            else:
                # ä½¿ç”¨FCå±‚è‡ªèº«çš„æ¢¯åº¦
                grad_values = stats.get(method)
                if grad_values is None:
                    print(f"è­¦å‘Š: å±‚ {layer_name} æ²¡æœ‰ {method} æ•°æ®ï¼Œè·³è¿‡")
                    continue
            
            # è½¬æ¢ä¸ºå¼ é‡è¿›è¡Œæ’åº
            grad_tensor = torch.tensor(grad_values, dtype=torch.float32)
            
            # æ’åºå¹¶é€‰æ‹©è¦å‰ªæçš„ç¥ç»å…ƒï¼ˆæ¢¯åº¦å€¼æœ€å°çš„ï¼‰
            sorted_indices = torch.argsort(grad_tensor)
            num_prune = int(len(grad_tensor) * ratio)
            
            for idx in sorted_indices[:num_prune]:
                # è·å–æƒé‡ä¿¡æ¯
                weight_info = 0.0
                for name, module in self.model.named_modules():
                    if name == layer_name and isinstance(module, nn.Linear):
                        weight_data = module.weight.data
                        weight_info = weight_data.abs().mean(dim=1).cpu().numpy()[idx.item()]
                        break
                
                # æ„é€ ç¥ç»å…ƒä¿¡æ¯
                neuron_info = {
                    'layer': layer_name,
                    'neuron_index': idx.item(),
                    'gradient_type': method,
                    'gradient_value': grad_values[idx.item()],
                    'weight_value': weight_info
                }
                
                # å¦‚æœä½¿ç”¨IFå±‚æ¢¯åº¦ï¼Œè®°å½•å¯¹åº”çš„IFå±‚ä¿¡æ¯
                if method == 'IF_output_grad_values':
                    neuron_info['if_layer'] = fc_to_if_mapping[layer_name]
                    neuron_info['if_gradient_value'] = grad_values[idx.item()]
                
                neurons_to_prune.append(neuron_info)
        
        return neurons_to_prune

    def print_gradient_analysis(self, gradient_stats):
        """æ‰“å°æ¢¯åº¦åˆ†æç»“æœ"""
        print("="*80)
        print("æ¢¯åº¦åˆ†æç»“æœ")
        print("="*80)
        
        if not gradient_stats:
            print("æ²¡æœ‰æ”¶é›†åˆ°æ¢¯åº¦æ•°æ®")
            return
        
        for layer_name, stats in gradient_stats.items():
            print(f"\nå±‚: {layer_name}")
            
            # æ‰“å°æƒé‡æ¢¯åº¦ç»Ÿè®¡
            if stats['weight_grad_values'] is not None:
                weight_grad = np.array(stats['weight_grad_values'])
                print(f"  æƒé‡æ¢¯åº¦ç»Ÿè®¡:")
                print(f"    å‡å€¼: {weight_grad.mean():.8f}")
                print(f"    æ ‡å‡†å·®: {weight_grad.std():.8f}")
                print(f"    æœ€å°å€¼: {weight_grad.min():.8f}")
                print(f"    æœ€å¤§å€¼: {weight_grad.max():.8f}")
                print(f"    ç¥ç»å…ƒæ•°é‡: {len(weight_grad)}")
            
            # æ‰“å°è¾“å…¥æ¢¯åº¦ç»Ÿè®¡
            if stats['input_grad_values'] is not None:
                input_grad = np.array(stats['input_grad_values'])
                print(f"  è¾“å…¥æ¢¯åº¦ç»Ÿè®¡:")
                print(f"    å‡å€¼: {input_grad.mean():.8f}")
                print(f"    æ ‡å‡†å·®: {input_grad.std():.8f}")
                print(f"    æœ€å°å€¼: {input_grad.min():.8f}")
                print(f"    æœ€å¤§å€¼: {input_grad.max():.8f}")
                print(f"    ç¥ç»å…ƒæ•°é‡: {len(input_grad)}")
            
            # æ‰“å°è¾“å‡ºæ¢¯åº¦ç»Ÿè®¡
            if stats['output_grad_values'] is not None:
                output_grad = np.array(stats['output_grad_values'])
                print(f"  è¾“å‡ºæ¢¯åº¦ç»Ÿè®¡:")
                print(f"    å‡å€¼: {output_grad.mean():.8f}")
                print(f"    æ ‡å‡†å·®: {output_grad.std():.8f}")
                print(f"    æœ€å°å€¼: {output_grad.min():.8f}")
                print(f"    æœ€å¤§å€¼: {output_grad.max():.8f}")
                print(f"    ç¥ç»å…ƒæ•°é‡: {len(output_grad)}")
            
            print("-"*60)
        
        # åˆ†æä½æ¢¯åº¦ç¥ç»å…ƒ
        print("\nä½æ¢¯åº¦ç¥ç»å…ƒåˆ†æ:")
        for method in ['weight_grad_values', 'input_grad_values', 'output_grad_values', 'IF_output_grad_values']:
            print(f"\nåŸºäº {method}:")
            for ratio in [0.05, 0.1, 0.2]:
                low_neurons = self.get_comprehensive_pruning_neurons(gradient_stats, ratio, method)
                print(f"  æ¢¯åº¦æœ€ä½ {ratio*100:.1f}% çš„ç¥ç»å…ƒæ•°é‡: {len(low_neurons)}")
                
                if low_neurons:
                    # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
                    layer_counts = {}
                    for neuron in low_neurons:
                        layer = neuron['layer']
                        if layer not in layer_counts:
                            layer_counts[layer] = 0
                        layer_counts[layer] += 1
                    
                    for layer, count in layer_counts.items():
                        if method == 'IF_output_grad_values':
                            # å¯¹äºIFæ¢¯åº¦æ–¹æ³•ï¼Œæ˜¾ç¤ºå¯¹åº”çš„IFå±‚ä¿¡æ¯
                            if layer == 'classifier.1':
                                if_layer = 'classifier.2'
                            elif layer == 'classifier.4':
                                if_layer = 'classifier.5'
                            else:
                                if_layer = 'æ— å¯¹åº”IFå±‚'
                            print(f"    {layer} (åŸºäº {if_layer}): {count} ä¸ª")
                        else:
                            print(f"    {layer}: {count} ä¸ª")
        
        print("="*80)
    
    def prune_neurons(self, neurons_to_prune):
        """æ‰§è¡Œç¥ç»å…ƒå‰ªæ"""
        # ç»Ÿè®¡æ¯å±‚å‰ªæçš„ç¥ç»å…ƒæ•°é‡
        layer_prune_count = {}
        
        for neuron_info in neurons_to_prune:
            layer_name = neuron_info['layer']
            neuron_idx = neuron_info['neuron_index']
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if layer_name not in layer_prune_count:
                layer_prune_count[layer_name] = 0
            layer_prune_count[layer_name] += 1
            
            # æ‰¾åˆ°å¯¹åº”çš„å±‚
            module = None
            for name, mod in self.model.named_modules():
                if name == layer_name and isinstance(mod, nn.Linear):
                    module = mod
                    break
            
            if module:
                # æ‰§è¡Œå‰ªæï¼šå°†ç¥ç»å…ƒçš„æƒé‡ç½®é›¶
                with torch.no_grad():
                    module.weight.data[neuron_idx] = 0
                    if module.bias is not None:
                        module.bias.data[neuron_idx] = 0
        
        # æ‰“å°æ¯å±‚å‰ªæç»Ÿè®¡ä¿¡æ¯
        print("\nå‰ªæç»Ÿè®¡ä¿¡æ¯:")
        print("="*60)
        print(f"{'å±‚åç§°':<30} {'å‰ªæç¥ç»å…ƒæ•°é‡':<15} {'æ€»ç¥ç»å…ƒæ•°é‡':<15}")
        print("-"*60)
        
        total_pruned = 0
        for layer_name, count in layer_prune_count.items():
            # è·å–è¯¥å±‚çš„æ€»ç¥ç»å…ƒæ•°é‡
            for name, module in self.model.named_modules():
                if name == layer_name and isinstance(module, nn.Linear):
                    total_neurons = module.out_features
                    print(f"{layer_name:<30} {count:<15} {total_neurons:<15}")
                    total_pruned += count
                    break
        
        print("-"*60)
        print(f"æ€»è®¡å‰ªæç¥ç»å…ƒæ•°é‡: {total_pruned}")
        print("="*60)
    
    def cleanup_hooks(self):
        """æ¸…ç†æ¢¯åº¦é’©å­"""
        for handle in self.weight_grad_hooks.values():
            handle.remove()
        for handle in self.tensor_grad_hooks.values():
            handle.remove()
        for handle in self.if_grad_hooks.values():
            handle.remove()
        self.weight_grad_hooks = {}
        self.tensor_grad_hooks = {}
        self.if_grad_hooks = {}
        self.gradient_records = {}

    def save_comprehensive_analysis(self, model, gradient_stats, timestamp, before_pruning_state=None):
        """
        ä¿å­˜æ¢¯åº¦ä¿¡æ¯åˆ°CSVæ–‡ä»¶
        
        å‚æ•°:
        model - æ¨¡å‹
        gradient_stats - æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯
        timestamp - æ—¶é—´æˆ³
        before_pruning_state - å‰ªæå‰çš„æ¨¡å‹çŠ¶æ€ï¼ˆä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨ï¼‰
        """
        # print("\nå¼€å§‹ä¿å­˜æ¢¯åº¦ä¿¡æ¯...")
        
        # ç¡®ä¿logç›®å½•å­˜åœ¨
        log_dir = "log_comprehensive_analysis"
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
            print(f"åˆ›å»ºç›®å½•: {log_dir}")
        
        # éå†æ‰€æœ‰å…¨è¿æ¥å±‚å’ŒIFå±‚
        for name, module in model.named_modules():
            # å¤„ç†FCå±‚
            if isinstance(module, nn.Linear):
                # print(f"ä¿å­˜FCå±‚ {name} çš„æ¢¯åº¦ä¿¡æ¯...")
                
                # æ£€æŸ¥è¯¥å±‚æ˜¯å¦æœ‰æ¢¯åº¦æ•°æ®
                if name not in gradient_stats:
                    print(f"  è­¦å‘Š: FCå±‚ {name} æ²¡æœ‰æ¢¯åº¦æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                # è·å–è¯¥å±‚çš„æ¢¯åº¦æ•°æ®
                layer_stats = gradient_stats[name]
                
                # åˆ†åˆ«ä¿å­˜æƒé‡æ¢¯åº¦å’Œè¾“å‡ºæ¢¯åº¦ï¼ˆå®ƒä»¬çš„ç»´åº¦ç›¸åŒï¼Œéƒ½æ˜¯è¾“å‡ºç¥ç»å…ƒæ•°é‡ï¼‰
                weight_grad = layer_stats.get('weight_grad_values')
                output_grad = layer_stats.get('output_grad_values')
                input_grad = layer_stats.get('input_grad_values')
                
                # ä¿å­˜è¾“å‡ºç›¸å…³æ¢¯åº¦ï¼ˆæƒé‡æ¢¯åº¦ + è¾“å‡ºæ¢¯åº¦ï¼‰
                if weight_grad is not None or output_grad is not None:
                    out_features = module.out_features
                    df_data = {'neuron_index': range(out_features)}
                    
                    if weight_grad is not None:
                        df_data['weight_grad_values'] = weight_grad
                    if output_grad is not None:
                        df_data['output_grad_values'] = output_grad
                    
                    df_output = pd.DataFrame(df_data)
                    filename_output = f"{name}_output_gradients_{timestamp}.csv"
                    filepath_output = os.path.join(log_dir, filename_output)
                    df_output.to_csv(filepath_output, index=False)
                    # print(f"  å·²ä¿å­˜è¾“å‡ºæ¢¯åº¦ä¿¡æ¯åˆ°: {filepath_output}")
                    # print(f"  è¾“å‡ºç¥ç»å…ƒæ•°é‡: {out_features}")
                
                # ä¿å­˜è¾“å…¥æ¢¯åº¦ï¼ˆå•ç‹¬ä¿å­˜ï¼Œå› ä¸ºç»´åº¦å¯èƒ½ä¸åŒï¼‰
                if input_grad is not None:
                    in_features = module.in_features
                    df_input_data = {
                        'neuron_index': range(in_features),
                        'input_grad_values': input_grad
                    }
                    
                    df_input = pd.DataFrame(df_input_data)
                    filename_input = f"{name}_input_gradients_{timestamp}.csv"
                    filepath_input = os.path.join(log_dir, filename_input)
                    df_input.to_csv(filepath_input, index=False)
                    # print(f"  å·²ä¿å­˜è¾“å…¥æ¢¯åº¦ä¿¡æ¯åˆ°: {filepath_input}")
                    # print(f"  è¾“å…¥ç¥ç»å…ƒæ•°é‡: {in_features}")
                
                if weight_grad is None and output_grad is None and input_grad is None:
                    print(f"  è­¦å‘Š: FCå±‚ {name} æ²¡æœ‰æœ‰æ•ˆçš„æ¢¯åº¦æ•°æ®")
            
            # å¤„ç†IFå±‚
            elif isinstance(module, IF) and name in ['classifier.2', 'classifier.5']:
                # print(f"ä¿å­˜IFå±‚ {name} çš„æ¢¯åº¦ä¿¡æ¯...")
                
                # æ£€æŸ¥è¯¥å±‚æ˜¯å¦æœ‰æ¢¯åº¦æ•°æ®
                if name not in gradient_stats:
                    print(f"  è­¦å‘Š: IFå±‚ {name} æ²¡æœ‰æ¢¯åº¦æ•°æ®ï¼Œè·³è¿‡")
                    continue
                
                # è·å–è¯¥å±‚çš„æ¢¯åº¦æ•°æ®
                layer_stats = gradient_stats[name]
                
                # ä¿å­˜IFå±‚ç‰¹æœ‰çš„æ•°æ®
                threshold_grad = layer_stats.get('threshold_grad_values')
                threshold_val = layer_stats.get('threshold_values')
                input_grad = layer_stats.get('input_grad_values')
                output_grad = layer_stats.get('output_grad_values')
                
                # ä¿å­˜é˜ˆå€¼ç›¸å…³ä¿¡æ¯
                if threshold_grad is not None or threshold_val is not None:
                    # åˆ›å»ºé˜ˆå€¼ä¿¡æ¯åˆ—è¡¨
                    thresh_data = []
                    
                    if threshold_grad is not None:
                        thresh_data.append({
                            'data_type': 'threshold_grad',
                            'value': threshold_grad
                        })
                    
                    if threshold_val is not None:
                        thresh_data.append({
                            'data_type': 'threshold_value', 
                            'value': threshold_val
                        })
                    
                    df_thresh = pd.DataFrame(thresh_data)
                    filename_thresh = f"{name}_threshold_info_{timestamp}.csv"
                    filepath_thresh = os.path.join(log_dir, filename_thresh)
                    df_thresh.to_csv(filepath_thresh, index=False)
                    # print(f"  å·²ä¿å­˜é˜ˆå€¼ä¿¡æ¯åˆ°: {filepath_thresh}")
                
                # ä¿å­˜ç¥ç»å…ƒæ¢¯åº¦ä¿¡æ¯ï¼ˆè¾“å…¥å’Œè¾“å‡ºæ¢¯åº¦ï¼‰
                if input_grad is not None or output_grad is not None:
                    neuron_features = len(input_grad) if input_grad is not None else len(output_grad)
                    neuron_data = {'neuron_index': range(neuron_features)}
                    
                    if input_grad is not None:
                        neuron_data['input_grad_values'] = input_grad
                    if output_grad is not None:
                        neuron_data['output_grad_values'] = output_grad
                    
                    df_neurons = pd.DataFrame(neuron_data)
                    filename_neurons = f"{name}_neuron_gradients_{timestamp}.csv"
                    filepath_neurons = os.path.join(log_dir, filename_neurons)
                    df_neurons.to_csv(filepath_neurons, index=False)
                    # print(f"  å·²ä¿å­˜ç¥ç»å…ƒæ¢¯åº¦ä¿¡æ¯åˆ°: {filepath_neurons}")
                    # print(f"  ç¥ç»å…ƒæ•°é‡: {neuron_features}")
                
                if threshold_grad is None and threshold_val is None and input_grad is None and output_grad is None:
                    print(f"  è­¦å‘Š: IFå±‚ {name} æ²¡æœ‰æœ‰æ•ˆçš„æ¢¯åº¦æ•°æ®")
        
        print("FCå±‚å’ŒIFå±‚æ¢¯åº¦ä¿¡æ¯ä¿å­˜å®Œæˆ!")

class OutputRedirector:
    """è¾“å‡ºé‡å®šå‘å™¨ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, 'w', encoding='utf-8')
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()

def evaluate_model(model, test_loader, criterion, device, seed=42):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åŠ è½½é¡ºåºä¸€è‡´
    seed_all(seed)
    
    # ä¿å­˜æ¨¡å‹åŸå§‹çŠ¶æ€
    original_state = {
        'training': model.training,
        'state_dict': model.state_dict().copy()
    }
    
    model.eval()  # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
    total_correct = 0
    total_samples = 0
    total_loss = 0.0
    
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for batch_idx, (images, labels) in enumerate(test_loader):
            images, labels = images.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # å¤„ç†SNNè¾“å‡º
            if len(outputs.shape) > 2:
                outputs = outputs.mean(0)  # å¯¹æ—¶é—´ç»´åº¦æ±‚å¹³å‡
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            total = labels.size(0)
            correct = (predicted == labels).sum().item()
            
            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            total_correct += correct
            total_samples += total
            total_loss += loss.item()
    
    # è®¡ç®—å¹³å‡å‡†ç¡®ç‡å’ŒæŸå¤±
    avg_accuracy = 100 * total_correct / total_samples
    avg_loss = total_loss / len(test_loader)
    # print(f"\nè¯„ä¼°å®Œæˆ:")
    # print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}% ({total_correct}/{total_samples})")
    print(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    # æ¢å¤æ¨¡å‹åŸå§‹çŠ¶æ€
    model.load_state_dict(original_state['state_dict'])
    if original_state['training']:
        model.train()
    else:
        model.eval()
    
    return avg_accuracy, avg_loss

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç¥ç»å…ƒæ¢¯åº¦åˆ†æå’Œå‰ªæ')
    parser.add_argument('--batch_size', default=200, type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', default='0', type=str, help='è®¾å¤‡')
    parser.add_argument('--seed', default=42, type=int, help='éšæœºç§å­')
    parser.add_argument('--mode', choices=['ann', 'snn'], default='snn', help='æ¨¡å¼')
    parser.add_argument('--num_batches', default=5, type=int, help='æ¢¯åº¦åˆ†æçš„æ‰¹æ¬¡æ•°')
    parser.add_argument('-r','--pruning_ratio', default=0.5, type=float, help='å‰ªææ¯”ä¾‹')
    parser.add_argument('--dataset', choices=['cifar10', 'cifar100'], default='cifar100', help='æ•°æ®é›†')
    parser.add_argument('--gradient_method', default='IF_output_grad_values', type=str, 
                       choices=['weight_grad_values', 'input_grad_values', 'output_grad_values', 'IF_output_grad_values'],
                       help='æ¢¯åº¦ç±»å‹é€‰æ‹©ï¼ˆIF_output_grad_valuesè¡¨ç¤ºä½¿ç”¨å¯¹åº”IFå±‚çš„è¾“å‡ºæ¢¯åº¦å¯¹FCå±‚å‰ªæï¼‰')
    parser.add_argument('--save_analysis', action='store_true', help='æ˜¯å¦ä¿å­˜æ¢¯åº¦åˆ†æç»“æœ')
    parser.add_argument('--print_analysis', action='store_true', help='æ˜¯å¦æ‰“å°è¯¦ç»†åˆ†æç»“æœ')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seed_all(args.seed)
    
    print(f"è®¾å¤‡: {device}, éšæœºç§å­: {args.seed}")
    print(f"åˆ†ææ¨¡å¼: {args.mode}")
    print(f"æ¢¯åº¦åˆ†ææ‰¹æ¬¡æ•°: {args.num_batches}")
    print(f"å‰ªææ¯”ä¾‹: {args.pruning_ratio}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"æ¢¯åº¦ç±»å‹: {args.gradient_method}")
    
    # åˆ›å»ºæ¨¡å‹
    model = modelpool('vgg16', args.dataset)
    
    # ç›´æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # model_path = '/root/autodl-tmp/0-ANN2SNN-Allinone/2-ANN_SNN_QCFS-SRP/cifar10-checkpoints/vgg16_wd[0.0005].pth'
    model_path = '/root/autodl-tmp/0-ANN2SNN-Allinone/2-ANN_SNN_QCFS-SRP/cifar100-checkpoints/vgg16_L[4].pth'
    
    print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    state_dict = torch.load(model_path, map_location=torch.device('cpu'))
    model.load_state_dict(state_dict)
    print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    if args.mode == 'snn':
        model.set_T(4)
        model.set_L(4)
        print("è®¾ç½®ä¸ºSNNæ¨¡å¼: T=8, L=4")
    else:
        model.set_T(0)
        model.set_L(4)
        print("è®¾ç½®ä¸ºANNæ¨¡å¼: T=0, L=4")
    
    model.to(device)
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½{args.dataset}æ•°æ®é›†...")
    train_loader, test_loader = datapool(args.dataset, args.batch_size)
    
    # ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°
    criterion = nn.CrossEntropyLoss()
    
    # ä¿å­˜æ¨¡å‹åˆå§‹çŠ¶æ€ï¼ˆæ·±æ‹·è´ï¼‰
    initial_state = {}
    for key, value in model.state_dict().items():
        initial_state[key] = value.clone().detach()
    
    # å‰ªæå‰è¯„ä¼°
    print("\nå‰ªæå‰è¯„ä¼°:")
    pre_accuracy, pre_loss = evaluate_model(model, test_loader, criterion, device, args.seed)
    
    # åˆ›å»ºç¥ç»å…ƒæ¢¯åº¦åˆ†æå™¨
    print("\n" + "="*80)
    print("å¼€å§‹ç¥ç»å…ƒæ¢¯åº¦åˆ†æ")
    print("="*80)
    
    analyzer = ComprehensiveNeuronAnalyzer(model)
    
    try:
        # åˆ†ææ¢¯åº¦åˆ†å¸ƒ
        gradient_stats = analyzer.analyze_gradients(
            train_loader, 
            criterion, 
            num_batches=args.num_batches
        )
        
        # æ‰“å°æ¢¯åº¦åˆ†æç»“æœ
        if args.print_analysis:
            analyzer.print_gradient_analysis(gradient_stats)
            analyzer.analyze_gradient_correlation(gradient_stats)
        
        # è·å–è¦å‰ªæçš„ç¥ç»å…ƒ
        neurons_to_prune = analyzer.get_comprehensive_pruning_neurons(
            gradient_stats, 
            ratio=args.pruning_ratio,
            method=args.gradient_method
        )
        
        print(f"\nåŸºäºæ¢¯åº¦åˆ†æï¼Œå°†å‰ªæ {len(neurons_to_prune)} ä¸ªç¥ç»å…ƒ")
        
        # æ‰§è¡Œå‰ªæ
        analyzer.prune_neurons(neurons_to_prune)
        
        # å‰ªæåè¯„ä¼°
        print("\nå‰ªæåè¯„ä¼°:")
        post_accuracy, post_loss = evaluate_model(model, test_loader, criterion, device, args.seed)
        
        # # æ‰“å°æ€§èƒ½å¯¹æ¯”
        # print("\næ€§èƒ½å¯¹æ¯”:")
        print(f"å‰ªæå‰: å‡†ç¡®ç‡ {pre_accuracy:.2f}%, æŸå¤± {pre_loss:.6f}")
        print(f"å‰ªæå: å‡†ç¡®ç‡ {post_accuracy:.2f}%, æŸå¤± {post_loss:.6f}")
        print(f"å‡†ç¡®ç‡å˜åŒ–: {post_accuracy - pre_accuracy:+.2f}%")
        print(f"æŸå¤±å˜åŒ–: {post_loss - pre_loss:+.6f}")
        
        # ä¿å­˜æ¢¯åº¦åˆ†æä¿¡æ¯
        if args.save_analysis:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            analyzer.save_comprehensive_analysis(model, gradient_stats, timestamp, initial_state)
                  
    finally:
        # æ¸…ç†æ¢¯åº¦é’©å­
        analyzer.cleanup_hooks()
    
    # print("\nâœ… ç¥ç»å…ƒæ¢¯åº¦åˆ†æå’Œå‰ªæå®Œæˆ!")
    # print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    # print("python 0614get_grad_ccc.py --mode snn --gradient_method weight_grad_values --print_analysis")
    # print("python 0614get_grad_ccc.py --mode snn --pruning_ratio 0.3 --save_analysis")
    # print("python 0614get_grad_ccc.py --mode ann --num_batches 10 --gradient_method output_grad_values")

if __name__ == "__main__":
    main() 
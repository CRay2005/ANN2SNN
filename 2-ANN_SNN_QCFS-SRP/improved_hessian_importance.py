#!/usr/bin/env python3
"""
æ”¹è¿›çš„Hessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨
è§£å†³æ•°å€¼ç²¾åº¦é—®é¢˜å’Œé‡‡æ ·ä¸è¶³é—®é¢˜
"""

import torch
import torch.nn as nn
import numpy as np
import time
from tqdm import tqdm


def get_params_grad(model):
    """è·å–æ¨¡å‹å‚æ•°å’Œå¯¹åº”çš„æ¢¯åº¦"""
    params = []
    grads = []
    for param in model.parameters():
        if not param.requires_grad or param.grad is None:
            continue
        params.append(param)
        grads.append(param.grad)
    return params, grads


def hessian_vector_product(gradsH, params, v, stop_criterion=False):
    """è®¡ç®—Hessianå‘é‡ä¹˜ç§¯ Hvï¼Œä½¿ç”¨é«˜ç²¾åº¦"""
    hv = torch.autograd.grad(gradsH, params, grad_outputs=v, 
                            only_inputs=True, retain_graph=not stop_criterion)
    return hv


class ImprovedHessianWeightImportance:
    """
    æ”¹è¿›çš„Hessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨
    ä¸“æ³¨äºå…¨è¿æ¥å±‚çš„Hessianåˆ†æ
    """
    
    def __init__(self, model, device='cuda', n_samples=500, use_double_precision=True):
        self.model = model
        self.device = device
        self.n_samples = n_samples
        self.use_double_precision = use_double_precision
        
        # å­˜å‚¨ç»“æœ
        self.layer_params = []
        self.hessian_traces = {}
        self.weight_importance = {}
        
        # å‡†å¤‡æ¨¡å‹å‚æ•°
        self._prepare_model()
    
    def _prepare_model(self):
        """å‡†å¤‡å…¨è¿æ¥å±‚å‚æ•°"""
        print("å‡†å¤‡å…¨è¿æ¥å±‚å‚æ•°...")
        
        # å¤„ç†å…¨è¿æ¥å±‚
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                # ä¿å­˜å‚æ•°ä¿¡æ¯
                self.layer_params.append((name, module.weight))
                print(f"æ³¨å†Œå…¨è¿æ¥å±‚: {name} (è¾“å…¥={module.in_features}, è¾“å‡º={module.out_features})")
    
    def compute_hessian_trace_hutchinson(self, data_loader, criterion):
        """ä½¿ç”¨æ”¹è¿›çš„Hutchinsonæ–¹æ³•è®¡ç®—Hessian trace"""
        print(f"ä½¿ç”¨æ”¹è¿›çš„Hutchinsonæ–¹æ³• (n_samples={self.n_samples}, é«˜ç²¾åº¦={self.use_double_precision})...")
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        self.model.train()
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
        for param in self.model.parameters():
            param.requires_grad = True
        
        # å¦‚æœä½¿ç”¨åŒç²¾åº¦ï¼Œè½¬æ¢æ¨¡å‹
        if self.use_double_precision:
            print("è½¬æ¢ä¸ºåŒç²¾åº¦æ¨¡å¼...")
            self.model = self.model.double()
        
        # è·å–å¤šä¸ªbatchæé«˜ç¨³å®šæ€§
        all_traces = {}
        for name, _ in self.layer_params:
            all_traces[name] = []
        
        batch_count = 0
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx >= 3:  # ä½¿ç”¨å‰3ä¸ªbatch
                break
                
            batch_count += 1
            data, target = data.to(self.device), target.to(self.device)
            
            if self.use_double_precision:
                data = data.double()
            
            print(f"\nå¤„ç†ç¬¬ {batch_idx + 1} ä¸ªbatch...")
            
            # å‰å‘ä¼ æ’­
            self.model.zero_grad()
            output = self.model(data)
            
            # å¤„ç†SNNè¾“å‡º
            if len(output.shape) > 2:
                output = output.mean(0)
            
            # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
            loss = criterion(output, target)
            loss.backward(create_graph=True)
            
            # è·å–å‚æ•°å’Œæ¢¯åº¦
            params = []
            gradsH = []
            valid_layers = []
            
            for name, param in self.layer_params:
                if param.grad is not None:
                    params.append(param)
                    gradsH.append(param.grad)
                    valid_layers.append((name, param))
                    print(f"å±‚ {name} çš„æ¢¯åº¦èŒƒæ•°: {param.grad.norm().item():.8f}")
                else:
                    print(f"è­¦å‘Š: å±‚ {name} æ²¡æœ‰æ¢¯åº¦")
            
            # Hutchinsoné‡‡æ · - å¢åŠ é‡‡æ ·æ•°
            batch_traces = {}
            for name, _ in valid_layers:
                batch_traces[name] = []
            
            print(f"å¼€å§‹Hutchinsoné‡‡æ · (æ‰¹æ¬¡ {batch_idx + 1})...")
            
            for sample_idx in range(self.n_samples):
                # ä½¿ç”¨ä¸åŒçš„éšæœºåˆ†å¸ƒæé«˜é‡‡æ ·è´¨é‡
                if sample_idx % 2 == 0:
                    # Rademacheråˆ†å¸ƒ
                    v = [torch.randint_like(p, high=2, device=self.device).float() * 2 - 1 
                         for p in params]
                else:
                    # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
                    v = [torch.randn_like(p, device=self.device) for p in params]
                
                if self.use_double_precision:
                    v = [vi.double() for vi in v]
                
                try:
                    # è®¡ç®—Hessian-å‘é‡ä¹˜ç§¯
                    Hv = hessian_vector_product(gradsH, params, v, 
                                              stop_criterion=(sample_idx == self.n_samples - 1))
                    
                    # è®¡ç®—traceå¹¶ä¿æŒé«˜ç²¾åº¦
                    for i, (name, param) in enumerate(valid_layers):
                        if i >= len(Hv):
                            continue
                        
                        # ç›´æ¥è®¡ç®— v^T * H * v
                        trace_val = torch.sum(v[i] * Hv[i]).item()
                        batch_traces[name].append(trace_val)
                    
                    # æ¸…ç†ä¸­é—´å˜é‡
                    del Hv
                    del v
                    
                except Exception as e:
                    print(f"  æ ·æœ¬ {sample_idx} è®¡ç®—å¤±è´¥: {e}")
                    continue
                
                if (sample_idx + 1) % 100 == 0:
                    print(f"  å®Œæˆ {sample_idx + 1}/{self.n_samples} æ¬¡é‡‡æ ·")
            
            # æ”¶é›†è¿™ä¸ªbatchçš„ç»“æœ
            for name in batch_traces:
                if len(batch_traces[name]) > 0:
                    avg_trace = np.mean(batch_traces[name])
                    all_traces[name].append(avg_trace)
                    print(f"  æ‰¹æ¬¡ {batch_idx + 1} - å±‚ {name}: å¹³å‡trace = {avg_trace:.8f}")
        
        # è®¡ç®—æ‰€æœ‰batchçš„æœ€ç»ˆå¹³å‡trace
        print(f"\nè®¡ç®— {batch_count} ä¸ªbatchçš„æœ€ç»ˆå¹³å‡...")
        for name in all_traces:
            if len(all_traces[name]) > 0:
                final_trace = np.mean(all_traces[name])
                trace_std = np.std(all_traces[name])
                self.hessian_traces[name] = [final_trace]
                print(f"å±‚ {name}: æœ€ç»ˆtrace = {final_trace:.8f} Â± {trace_std:.8f}")
            else:
                self.hessian_traces[name] = [0.0]
                print(f"å±‚ {name}: æ— æœ‰æ•ˆtraceï¼Œè®¾ä¸º0")
    
    def collect_neuron_importance(self):
        """
        æ”¶é›†æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§æ•°æ®
        
        è¿”å›:
        neuron_importance_list - åŒ…å«æ¯ä¸ªç¥ç»å…ƒé‡è¦æ€§ä¿¡æ¯çš„åˆ—è¡¨
        """
        print("\næ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®...")
        neuron_importance_list = []
        
        for name, param in self.layer_params:
            if name not in self.hessian_traces:
                print(f"è­¦å‘Š: {name} æ²¡æœ‰Hessian trace")
                continue
            
            # æ£€æŸ¥æ¢¯åº¦
            if param.grad is None:
                print(f"è­¦å‘Š: {name} æ²¡æœ‰æ¢¯åº¦")
                continue
                
            # æ£€æŸ¥Hessian trace
            hessian_trace = self.hessian_traces[name][0]
            if abs(hessian_trace) < 1e-10:
                print(f"è­¦å‘Š: {name} çš„Hessian traceæ¥è¿‘0")
            
            weight_norm_sq = param.norm(p=2) ** 2
            param_count = param.numel()
            
            # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§
            if param_count > 0:
                base_importance = hessian_trace * weight_norm_sq.item() / param_count
                
                # å¯¹æ¯ä¸ªç¥ç»å…ƒè®¡ç®—é‡è¦æ€§
                for neuron_idx in range(param.size(0)):  # éå†è¾“å‡ºç¥ç»å…ƒ
                    # è·å–è¯¥ç¥ç»å…ƒçš„æƒé‡èŒƒæ•°
                    neuron_weight_norm = param[neuron_idx].norm(p=2) ** 2
                    # è®¡ç®—è¯¥ç¥ç»å…ƒçš„é‡è¦æ€§
                    neuron_importance = base_importance * neuron_weight_norm.item()
                    
                    # æ·»åŠ åˆ°åˆ—è¡¨
                    neuron_importance_list.append({
                        'layer': name,
                        'neuron_id': neuron_idx,
                        'importance': neuron_importance
                    })
                    
                    if neuron_idx < 5:  # åªæ‰“å°å‰5ä¸ªç¥ç»å…ƒçš„ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
                        print(f"å±‚ {name} ç¥ç»å…ƒ {neuron_idx}: é‡è¦æ€§ = {neuron_importance:.8f}")
        
        # æŒ‰é‡è¦æ€§æ’åº
        neuron_importance_list.sort(key=lambda x: x['importance'], reverse=True)
        
        print(f"\næ€»å…±æ”¶é›†äº† {len(neuron_importance_list)} ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§æ•°æ®")
        return neuron_importance_list
    
    def get_pruning_candidates(self, neuron_importance_list, pruning_ratio=0.3):
        """
        è·å–å‰ªæå€™é€‰ï¼ŒåŸºäºç¥ç»å…ƒé‡è¦æ€§æ’åº
        
        å‚æ•°:
        neuron_importance_list - ç¥ç»å…ƒé‡è¦æ€§åˆ—è¡¨
        pruning_ratio - è¦å‰ªæçš„ç¥ç»å…ƒæ¯”ä¾‹
        
        è¿”å›:
        pruning_candidates - å‰ªæå€™é€‰åˆ—è¡¨
        """
        print(f"\nç”Ÿæˆå‰ªæå€™é€‰ (å‰ªææ¯”ä¾‹: {pruning_ratio})...")
        
        # æŒ‰é‡è¦æ€§æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
        sorted_neurons = sorted(neuron_importance_list, key=lambda x: x['importance'])
        
        # é€‰æ‹©è¦å‰ªæçš„ç¥ç»å…ƒ
        num_to_prune = int(len(sorted_neurons) * pruning_ratio)
        pruning_candidates = sorted_neurons[:num_to_prune]
        
        # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
        layer_counts = {}
        for neuron in pruning_candidates:
            layer = neuron['layer']
            if layer not in layer_counts:
                layer_counts[layer] = 0
            layer_counts[layer] += 1
        
        print(f"é€‰æ‹©äº† {len(pruning_candidates)} ä¸ªç¥ç»å…ƒè¿›è¡Œå‰ªæ:")
        for layer, count in layer_counts.items():
            print(f"  {layer}: {count} ä¸ªç¥ç»å…ƒ")
        
        return pruning_candidates

    def run_full_analysis(self, data_loader, criterion):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("="*80)
        print("ğŸš€ å…¨è¿æ¥å±‚Hessiané‡è¦æ€§åˆ†æ")
        print("="*80)
        
        start_time = time.time()
        
        try:
            # è®¡ç®—Hessian trace
            print("\n1. è®¡ç®—Hessian trace...")
            self.compute_hessian_trace_hutchinson(data_loader, criterion)
            
            # æ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®
            print("\n2. æ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®...")
            neuron_importance_list = self.collect_neuron_importance()
            
            # ç”Ÿæˆå‰ªæå€™é€‰
            print("\n3. ç”Ÿæˆå‰ªæå€™é€‰...")
            pruning_candidates = self.get_pruning_candidates(neuron_importance_list)
            
            total_time = time.time() - start_time
            print(f"\nâœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print("="*80)
            
            return {
                'neuron_importance_list': neuron_importance_list,
                'hessian_traces': self.hessian_traces,
                'pruning_candidates': pruning_candidates
            }
        finally:
            # æ¸…ç†
            if self.use_double_precision:
                self.model = self.model.float()


# æµ‹è¯•è„šæœ¬
if __name__ == "__main__":
    print("æµ‹è¯•æ”¹è¿›çš„Hessiané‡è¦æ€§è®¡ç®—å™¨...")
    
    from Models import modelpool
    from Preprocess import datapool
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºæ¨¡å‹
    model = modelpool('vgg16', 'cifar10')
    model.set_L(8)
    model.set_T(0)  # ANNæ¨¡å¼
    model.to(device)
    
    # åŠ è½½æ•°æ®
    train_loader, _ = datapool('cifar10', 16)
    
    # åˆ›å»ºæ”¹è¿›çš„è®¡ç®—å™¨
    hessian_calc = ImprovedHessianWeightImportance(
        model=model,
        device=device,
        n_samples=200,  # å¢åŠ é‡‡æ ·æ•°
        use_double_precision=True  # ä½¿ç”¨åŒç²¾åº¦
    )
    
    # è¿è¡Œåˆ†æ
    criterion = nn.CrossEntropyLoss()
    results = hessian_calc.run_full_analysis(train_loader, criterion)
    
    print("\nğŸ¯ åˆ†æç»“æœæ€»ç»“:")
    print(f"ç¥ç»å…ƒæ€»æ•°: {len(results['neuron_importance_list'])}")
    if len(results['neuron_importance_list']) > 0:
        print("âœ… æˆåŠŸè®¡ç®—å‡ºç¥ç»å…ƒé‡è¦æ€§å€¼ï¼")
    else:
        print("âŒ æœªèƒ½è®¡ç®—å‡ºç¥ç»å…ƒé‡è¦æ€§å€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•") 
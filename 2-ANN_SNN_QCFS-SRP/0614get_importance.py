#!/usr/bin/env python3
"""
æ”¹è¿›çš„Hessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨
è§£å†³æ•°å€¼ç²¾åº¦é—®é¢˜å’Œé‡‡æ ·ä¸è¶³é—®é¢˜
"""

import torch
import torch.nn as nn
import numpy as np
import time
from tqdm import tqdm
from Models import modelpool
from Preprocess import datapool
import pandas as pd
from datetime import datetime
import argparse
import os
import sys
from utils import seed_all

class OutputRedirector:
    """è¾“å‡ºé‡å®šå‘å™¨ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, 'w', encoding='utf-8')
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()

def get_params_grad(model):
    """è·å–æ¨¡å‹å‚æ•°å’Œå¯¹åº”çš„æ¢¯åº¦"""
    params = []
    grads = []
    for param in model.parameters():
        if not param.requires_grad or param.grad is None:
            continue
        params.append(param)
        grads.append(param.grad)
    return params, grads


def hessian_vector_product(gradsH, params, v, stop_criterion=False):
    """è®¡ç®—Hessianå‘é‡ä¹˜ç§¯ Hvï¼Œä½¿ç”¨é«˜ç²¾åº¦"""
    hv = torch.autograd.grad(gradsH, params, grad_outputs=v, 
                            only_inputs=True, retain_graph=not stop_criterion)
    return hv


class ImprovedHessianWeightImportance:
    """
    æ”¹è¿›çš„Hessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨
    ä¸“æ³¨äºå…¨è¿æ¥å±‚çš„Hessianåˆ†æ
    """
    
    def __init__(self, model, device='cuda', n_samples=500, use_double_precision=True):
        self.model = model
        self.device = device
        self.n_samples = n_samples
        self.use_double_precision = use_double_precision
        
        # å­˜å‚¨ç»“æœ
        self.layer_params = []
        self.hessian_traces = {}
        self.weight_importance = {}
        
        # å‡†å¤‡æ¨¡å‹å‚æ•°
        self._prepare_model()
    
    def _prepare_model(self):
        """å‡†å¤‡å…¨è¿æ¥å±‚å‚æ•°"""
        print("å‡†å¤‡å…¨è¿æ¥å±‚å‚æ•°...")
        
        # å¤„ç†å…¨è¿æ¥å±‚
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                # ä¿å­˜å‚æ•°ä¿¡æ¯
                self.layer_params.append((name, module.weight))
                print(f"æ³¨å†Œå…¨è¿æ¥å±‚: {name} (è¾“å…¥={module.in_features}, è¾“å‡º={module.out_features})")
    
    def compute_hessian_trace_hutchinson(self, data_loader, criterion):
        """ä½¿ç”¨æ”¹è¿›çš„Hutchinsonæ–¹æ³•è®¡ç®—Hessian trace"""
        print(f"ä½¿ç”¨æ”¹è¿›çš„Hutchinsonæ–¹æ³• (n_samples={self.n_samples}, é«˜ç²¾åº¦={self.use_double_precision})...")
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        self.model.train()
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ
        for param in self.model.parameters():
            param.requires_grad = True
        
        # å¦‚æœä½¿ç”¨åŒç²¾åº¦ï¼Œè½¬æ¢æ¨¡å‹
        if self.use_double_precision:
            print("è½¬æ¢ä¸ºåŒç²¾åº¦æ¨¡å¼...")
            self.model = self.model.double()
        
        # è·å–å¤šä¸ªbatchæé«˜ç¨³å®šæ€§
        all_traces = {}
        for name, _ in self.layer_params:
            all_traces[name] = []
        
        batch_count = 0
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx >= 3:  # ä½¿ç”¨å‰3ä¸ªbatch
                break
                
            batch_count += 1
            data, target = data.to(self.device), target.to(self.device)
            
            if self.use_double_precision:
                data = data.double()
            
            print(f"\nå¤„ç†ç¬¬ {batch_idx + 1} ä¸ªbatch...")
            
            # å‰å‘ä¼ æ’­
            self.model.zero_grad()
            output = self.model(data)
            
            # å¤„ç†SNNè¾“å‡º
            if len(output.shape) > 2:
                output = output.mean(0)
            
            # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
            loss = criterion(output, target)
            loss.backward(create_graph=True)
            
            # è·å–å‚æ•°å’Œæ¢¯åº¦
            params = []
            gradsH = []
            valid_layers = []
            
            for name, param in self.layer_params:
                if param.grad is not None:
                    params.append(param)
                    gradsH.append(param.grad)
                    valid_layers.append((name, param))
                    # print(f"å±‚ {name} çš„æ¢¯åº¦èŒƒæ•°: {param.grad.norm().item():.8f}")
                else:
                    print(f"è­¦å‘Š: å±‚ {name} æ²¡æœ‰æ¢¯åº¦")
            
            # Hutchinsoné‡‡æ · - å¢åŠ é‡‡æ ·æ•°
            batch_traces = {}
            for name, _ in valid_layers:
                batch_traces[name] = []
            
            print(f"å¼€å§‹Hutchinsoné‡‡æ · (æ‰¹æ¬¡ {batch_idx + 1})...")
            
            for sample_idx in range(self.n_samples):
                # ä½¿ç”¨ä¸åŒçš„éšæœºåˆ†å¸ƒæé«˜é‡‡æ ·è´¨é‡
                if sample_idx % 2 == 0:
                    # Rademacheråˆ†å¸ƒ
                    v = [torch.randint_like(p, high=2, device=self.device).float() * 2 - 1 
                         for p in params]
                else:
                    # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
                    v = [torch.randn_like(p, device=self.device) for p in params]
                
                if self.use_double_precision:
                    v = [vi.double() for vi in v]
                
                try:
                    # è®¡ç®—Hessian-å‘é‡ä¹˜ç§¯
                    Hv = hessian_vector_product(gradsH, params, v, 
                                              stop_criterion=(sample_idx == self.n_samples - 1))
                    
                    # è®¡ç®—traceå¹¶ä¿æŒé«˜ç²¾åº¦
                    for i, (name, param) in enumerate(valid_layers):
                        if i >= len(Hv):
                            continue
                        
                        # ç›´æ¥è®¡ç®— v^T * H * v
                        trace_val = torch.sum(v[i] * Hv[i]).item()
                        batch_traces[name].append(trace_val)
                    
                    # æ¸…ç†ä¸­é—´å˜é‡
                    del Hv
                    del v
                    
                except Exception as e:
                    print(f"  æ ·æœ¬ {sample_idx} è®¡ç®—å¤±è´¥: {e}")
                    continue
                
                # if (sample_idx + 1) % 100 == 0:
                #     print(f"  å®Œæˆ {sample_idx + 1}/{self.n_samples} æ¬¡é‡‡æ ·")
            
            # æ”¶é›†è¿™ä¸ªbatchçš„ç»“æœ
            for name in batch_traces:
                if len(batch_traces[name]) > 0:
                    avg_trace = np.mean(batch_traces[name])
                    all_traces[name].append(avg_trace)
                    # print(f"  æ‰¹æ¬¡ {batch_idx + 1} - å±‚ {name}: å¹³å‡trace = {avg_trace:.8f}")
        
        # è®¡ç®—æ‰€æœ‰batchçš„æœ€ç»ˆå¹³å‡trace
        print(f"\nè®¡ç®— {batch_count} ä¸ªbatchçš„æœ€ç»ˆå¹³å‡...")
        for name in all_traces:
            if len(all_traces[name]) > 0:
                final_trace = np.mean(all_traces[name])
                trace_std = np.std(all_traces[name])
                self.hessian_traces[name] = [final_trace]
                # print(f"å±‚ {name}: æœ€ç»ˆtrace = {final_trace:.8f} Â± {trace_std:.8f}")
            else:
                self.hessian_traces[name] = [0.0]
                print(f"å±‚ {name}: æ— æœ‰æ•ˆtraceï¼Œè®¾ä¸º0")
    
    def collect_neuron_importance(self):
        """
        æ”¶é›†æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§æ•°æ®
        
        è¿”å›:
        neuron_importance_list - åŒ…å«æ¯ä¸ªç¥ç»å…ƒé‡è¦æ€§ä¿¡æ¯çš„åˆ—è¡¨
        """
        print("\næ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®...")
        neuron_importance_list = []
        
        for name, param in self.layer_params:
            if name not in self.hessian_traces:
                print(f"è­¦å‘Š: {name} æ²¡æœ‰Hessian trace")
                continue
            
            # æ£€æŸ¥æ¢¯åº¦
            if param.grad is None:
                print(f"è­¦å‘Š: {name} æ²¡æœ‰æ¢¯åº¦")
                continue
                
            # æ£€æŸ¥Hessian trace
            hessian_trace = self.hessian_traces[name][0]
            if abs(hessian_trace) < 1e-10:
                print(f"è­¦å‘Š: {name} çš„Hessian traceæ¥è¿‘0")
            
            weight_norm_sq = param.norm(p=2) ** 2
            param_count = param.numel()
            
            # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§
            if param_count > 0:
                base_importance = hessian_trace * weight_norm_sq.item() / param_count
                
                # å¯¹æ¯ä¸ªç¥ç»å…ƒè®¡ç®—é‡è¦æ€§
                for neuron_idx in range(param.size(0)):  # éå†è¾“å‡ºç¥ç»å…ƒ
                    # è·å–è¯¥ç¥ç»å…ƒçš„æƒé‡èŒƒæ•°
                    neuron_weight_norm = param[neuron_idx].norm(p=2) ** 2
                    # è®¡ç®—è¯¥ç¥ç»å…ƒçš„é‡è¦æ€§
                    neuron_importance = base_importance * neuron_weight_norm.item()
                    
                    # æ·»åŠ åˆ°åˆ—è¡¨
                    neuron_importance_list.append({
                        'layer': name,
                        'neuron_id': neuron_idx,
                        'importance': neuron_importance
                    })
                    
                    # if neuron_idx < 5:  # åªæ‰“å°å‰5ä¸ªç¥ç»å…ƒçš„ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
                    #     print(f"å±‚ {name} ç¥ç»å…ƒ {neuron_idx}: é‡è¦æ€§ = {neuron_importance:.8f}")
        
        # æŒ‰é‡è¦æ€§æ’åº
        neuron_importance_list.sort(key=lambda x: x['importance'], reverse=True)
        
        print(f"\næ€»å…±æ”¶é›†äº† {len(neuron_importance_list)} ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§æ•°æ®")
        return neuron_importance_list
    
    def get_pruning_candidates(self, neuron_importance_list, pruning_ratio=0.5):
        """
        è·å–å‰ªæå€™é€‰ï¼ŒåŸºäºç¥ç»å…ƒé‡è¦æ€§æ’åº
        
        å‚æ•°:
        neuron_importance_list - ç¥ç»å…ƒé‡è¦æ€§åˆ—è¡¨
        pruning_ratio - è¦å‰ªæçš„ç¥ç»å…ƒæ¯”ä¾‹
        
        è¿”å›:
        pruning_candidates - å‰ªæå€™é€‰åˆ—è¡¨
        """
        print(f"\nç”Ÿæˆå‰ªæå€™é€‰ (å‰ªææ¯”ä¾‹: {pruning_ratio})...")
        
        # æŒ‰é‡è¦æ€§æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
        sorted_neurons = sorted(neuron_importance_list, key=lambda x: x['importance'])
        
        # é€‰æ‹©è¦å‰ªæçš„ç¥ç»å…ƒ
        num_to_prune = int(len(sorted_neurons) * pruning_ratio)
        pruning_candidates = sorted_neurons[:num_to_prune]
        
        # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
        layer_counts = {}
        for neuron in pruning_candidates:
            layer = neuron['layer']
            if layer not in layer_counts:
                layer_counts[layer] = 0
            layer_counts[layer] += 1
        
        print(f"é€‰æ‹©äº† {len(pruning_candidates)} ä¸ªç¥ç»å…ƒè¿›è¡Œå‰ªæ:")
        for layer, count in layer_counts.items():
            print(f"  {layer}: {count} ä¸ªç¥ç»å…ƒ")
        
        return pruning_candidates

    def run_full_analysis(self, data_loader, criterion, pruning_ratio):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("="*80)
        print("ğŸš€ å…¨è¿æ¥å±‚Hessiané‡è¦æ€§åˆ†æ")
        print("="*80)
        
        start_time = time.time()
        
        try:
            # è®¡ç®—Hessian trace
            print("\n1. è®¡ç®—Hessian trace...")
            self.compute_hessian_trace_hutchinson(data_loader, criterion)
            
            # æ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®
            print("\n2. æ”¶é›†ç¥ç»å…ƒé‡è¦æ€§æ•°æ®...")
            neuron_importance_list = self.collect_neuron_importance()
            
            # ç”Ÿæˆå‰ªæå€™é€‰
            print("\n3. ç”Ÿæˆå‰ªæå€™é€‰...")
            pruning_candidates = self.get_pruning_candidates(neuron_importance_list,pruning_ratio)
            
            total_time = time.time() - start_time
            print(f"\nâœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print("="*80)
            
            return {
                'neuron_importance_list': neuron_importance_list,
                'hessian_traces': self.hessian_traces,
                'pruning_candidates': pruning_candidates
            }
        finally:
            # æ¸…ç†
            if self.use_double_precision:
                self.model = self.model.float()


def evaluate_model(model, test_loader, criterion, device, seed=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    if seed is not None:
        seed_all(seed)
        
    model.eval()
    total_correct = 0
    total_samples = 0
    total_loss = 0.0
    
    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(test_loader):
            images, labels = images.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # å¤„ç†SNNè¾“å‡º
            if len(outputs.shape) > 2:
                outputs = outputs.mean(0)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            total = labels.size(0)
            correct = (predicted == labels).sum().item()
            
            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            total_correct += correct
            total_samples += total
            total_loss += loss.item()
    
    # è®¡ç®—å¹³å‡å‡†ç¡®ç‡å’ŒæŸå¤±
    avg_accuracy = 100 * total_correct / total_samples
    avg_loss = total_loss / len(test_loader)
    print(f"\nè¯„ä¼°å®Œæˆ:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}% ({total_correct}/{total_samples})")
    print(f"  å¹³å‡æŸå¤±: {avg_loss:.6f}")
    
    return avg_accuracy, avg_loss

def prune_neurons(model, pruning_candidates):
    """æ‰§è¡Œç¥ç»å…ƒå‰ªæ"""
    print("\næ‰§è¡Œç¥ç»å…ƒå‰ªæ...")
    for neuron_info in pruning_candidates:
        layer_name = neuron_info['layer']
        neuron_idx = neuron_info['neuron_id']
        
        # æ‰¾åˆ°å¯¹åº”çš„å±‚
        module = None
        for name, mod in model.named_modules():
            if name == layer_name and isinstance(mod, nn.Linear):
                module = mod
                break
        
        if module:
            # æ‰§è¡Œå‰ªæï¼šå°†ç¥ç»å…ƒçš„æƒé‡ç½®é›¶
            with torch.no_grad():
                module.weight.data[neuron_idx] = 0
                if module.bias is not None:
                    module.bias.data[neuron_idx] = 0

def save_results_to_csv(neuron_importance_list, timestamp):
    """ä¿å­˜ç¥ç»å…ƒé‡è¦æ€§æ•°æ®åˆ°CSVæ–‡ä»¶"""
    print("\nä¿å­˜ç¥ç»å…ƒé‡è¦æ€§æ•°æ®...")
    
    # æŒ‰å±‚åˆ†ç»„
    layer_data = {}
    for neuron in neuron_importance_list:
        layer = neuron['layer']
        if layer not in layer_data:
            layer_data[layer] = []
        layer_data[layer].append({
            'neuron_id': neuron['neuron_id'],
            'importance': neuron['importance']
        })
    
    # ä¸ºæ¯å±‚åˆ›å»ºCSVæ–‡ä»¶
    for layer_name, neurons in layer_data.items():
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(neurons)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"importance_analysis_{layer_name}_{timestamp}.csv"
        
        # ä¿å­˜åˆ°CSV
        df.to_csv(filename, index=False)
        print(f"å·²ä¿å­˜{layer_name}å±‚çš„é‡è¦æ€§ä¿¡æ¯åˆ°: {filename}")
        print(f"  ç¥ç»å…ƒæ•°é‡: {len(neurons)}")
        print(f"  æ¯ä¸ªç¥ç»å…ƒåŒ…å«1ä¸ªé‡è¦æ€§å€¼")

def main():
    """ä¸»å‡½æ•°"""
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ç¥ç»å…ƒé‡è¦æ€§åˆ†æä¸å‰ªæ')
    parser.add_argument('--batch_size', default=200, type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', default='0', type=str, help='è®¾å¤‡')
    parser.add_argument('--seed', default=42, type=int, help='éšæœºç§å­')
    parser.add_argument('--mode', choices=['ann', 'snn'], default='snn', help='æ¨¡å¼')
    parser.add_argument('--n_samples', default=200, type=int, help='Hessiané‡‡æ ·æ•°é‡')
    parser.add_argument('-r','--pruning_ratio', default=0.5, type=float, help='å‰ªææ¯”ä¾‹')
    parser.add_argument('--dataset', choices=['cifar10', 'cifar100'], default='cifar100', help='æ•°æ®é›†')
    parser.add_argument('--use_double_precision', action='store_true', help='æ˜¯å¦ä½¿ç”¨åŒç²¾åº¦è®¡ç®—')
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºé‡å®šå‘ï¼ˆé»˜è®¤ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"./log/importance_analysis_{args.mode}_{timestamp}.txt"
    output_redirector = OutputRedirector(filename)
    sys.stdout = output_redirector
    print(f"è¾“å‡ºå°†ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seed_all(args.seed)
    
    print(f"è®¾å¤‡: {device}, éšæœºç§å­: {args.seed}")
    print(f"åˆ†ææ¨¡å¼: {args.mode}")
    print(f"Hessiané‡‡æ ·æ•°é‡: {args.n_samples}")
    print(f"å‰ªææ¯”ä¾‹: {args.pruning_ratio}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"ä½¿ç”¨åŒç²¾åº¦: {args.use_double_precision}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºVGG16æ¨¡å‹...")
    model = modelpool('vgg16', args.dataset)
    

    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if args.dataset == 'cifar10':
        model_path = '/root/autodl-tmp/0-ANN2SNN-Allinone/2-ANN_SNN_QCFS-SRP/cifar10-checkpoints/vgg16_wd[0.0005].pth'
    else:
        model_path = '/root/autodl-tmp/0-ANN2SNN-Allinone/2-ANN_SNN_QCFS-SRP/cifar100-checkpoints/vgg16_L[4].pth'
    
    print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    state_dict = torch.load(model_path, map_location=torch.device('cpu'))
    model.load_state_dict(state_dict)
    print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")

    # è®¾ç½®æ¨¡å‹æ¨¡å¼
    if args.mode == 'snn':
        model.set_T(8)
        model.set_L(4)
        print("è®¾ç½®ä¸ºSNNæ¨¡å¼")
    else:
        model.set_T(0)
        model.set_L(4)
        print("è®¾ç½®ä¸ºANNæ¨¡å¼")


    model.to(device)


    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½{args.dataset}æ•°æ®é›†...")
    train_loader, test_loader = datapool(args.dataset, args.batch_size)

    criterion = nn.CrossEntropyLoss()

    # å‰ªæå‰è¯„ä¼°
    print("\nå‰ªæå‰è¯„ä¼°:")
    pre_accuracy, pre_loss = evaluate_model(model, test_loader, criterion, device, args.seed)
        
    # åˆ›å»ºæ”¹è¿›çš„è®¡ç®—å™¨
    hessian_calc = ImprovedHessianWeightImportance(
        model=model,
        device=device,
        n_samples=args.n_samples,
        use_double_precision=args.use_double_precision
    )
    
    # è¿è¡Œåˆ†æ

    results = hessian_calc.run_full_analysis(train_loader, criterion,args.pruning_ratio)
    
    # è·å–å‰ªæå€™é€‰
    pruning_candidates = results['pruning_candidates']
    
    # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
    layer_counts = {}
    for neuron in pruning_candidates:
        layer = neuron['layer']
        if layer not in layer_counts:
            layer_counts[layer] = 0
        layer_counts[layer] += 1
    
    print("\nå»ºè®®å‰ªæçš„ç¥ç»å…ƒç»Ÿè®¡:")
    for layer, count in layer_counts.items():
        print(f"  {layer}: {count} ä¸ªç¥ç»å…ƒ")
    

    # æ‰§è¡Œå‰ªæ
    prune_neurons(model, pruning_candidates)
    
    # å‰ªæåè¯„ä¼°
    print("\nå‰ªæåè¯„ä¼°:")
    post_accuracy, post_loss = evaluate_model(model, test_loader, criterion, device, args.seed)
    
    # # ä¿å­˜ç»“æœ
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # save_results_to_csv(results['neuron_importance_list'], timestamp)
    
    # # ä¿å­˜è¯„ä¼°ç»“æœ
    # eval_results = {
    #     'pre_pruning': {
    #         'accuracy': pre_accuracy,
    #         'loss': pre_loss
    #     },
    #     'post_pruning': {
    #         'accuracy': post_accuracy,
    #         'loss': post_loss
    #     }
    # }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    # torch.save(eval_results, f'evaluation_results_{timestamp}.pt')
    # print(f"\nğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° evaluation_results_{timestamp}.pt")
    
    return results


if __name__ == "__main__":
    results = main() 
#!/usr/bin/env python3
"""
é›†æˆæ”¹è¿›Hessiané‡è¦æ€§åˆ†æçš„è®­ç»ƒæµ‹è¯•
"""

import argparse
import torch
import torch.nn as nn
from Models import modelpool
from Preprocess import datapool
from improved_hessian_importance import ImprovedHessianWeightImportance


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ æµ‹è¯•æ”¹è¿›çš„Hessiané‡è¦æ€§è®¡ç®—å™¨")
    print("="*80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = modelpool('vgg16', 'cifar10')
    model.set_L(8)
    model.set_T(0)  # ANNæ¨¡å¼
    model.to(device)
    
    # åŠ è½½æ•°æ®
    train_loader, _ = datapool('cifar10', 16)
    
    # åˆ›å»ºæ”¹è¿›çš„è®¡ç®—å™¨
    hessian_calc = ImprovedHessianWeightImportance(
        model=model,
        device=device,
        n_samples=200,  # å¢åŠ é‡‡æ ·æ•°
        use_double_precision=True  # ä½¿ç”¨åŒç²¾åº¦
    )
    
    # è¿è¡Œåˆ†æ
    criterion = nn.CrossEntropyLoss()
    results = hessian_calc.run_full_analysis(train_loader, criterion)
    
    # è·å–å‰ªæå€™é€‰
    pruning_candidates = results['pruning_candidates']
    
    # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
    layer_counts = {}
    for neuron in pruning_candidates:
        layer = neuron['layer']
        if layer not in layer_counts:
            layer_counts[layer] = 0
        layer_counts[layer] += 1
    
    print("\nå»ºè®®å‰ªæçš„ç¥ç»å…ƒç»Ÿè®¡:")
    for layer, count in layer_counts.items():
        print(f"  {layer}: {count} ä¸ªç¥ç»å…ƒ")
    
    return results


if __name__ == "__main__":
    results = main()
    
    # å¯ä»¥ä¿å­˜ç»“æœä¾›åç»­ä½¿ç”¨
    torch.save(results, 'hessian_importance_results.pt')
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° hessian_importance_results.pt") 
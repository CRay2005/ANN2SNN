 #!/usr/bin/env python3
"""
æµ‹è¯•åŸºäºHSTNNçš„Hessianæƒé‡é‡è¦æ€§è®¡ç®—
"""

import argparse
import os
import torch
import torch.nn as nn
from Models import modelpool
from Preprocess import datapool
from hessian_importance import HessianWeightImportance


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•Hessianæƒé‡é‡è¦æ€§è®¡ç®—')
    parser.add_argument('-data', '--dataset', default='cifar10', type=str, help='æ•°æ®é›†')
    parser.add_argument('-arch', '--model', default='vgg16', type=str, help='æ¨¡å‹æ¶æ„')
    parser.add_argument('-T', '--time', default=4, type=int, help='SNNæ—¶é—´æ­¥é•¿')
    parser.add_argument('-L', '--L', default=8, type=int, help='é‡åŒ–çº§åˆ«')
    parser.add_argument('-b', '--batch_size', default=32, type=int, help='æ‰¹å¤§å°')
    parser.add_argument('--device', default='0', type=str, help='GPUè®¾å¤‡')
    parser.add_argument('--n_samples', default=50, type=int, help='Hutchinsoné‡‡æ ·æ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, test_loader = datapool(args.dataset, args.batch_size)
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = modelpool(args.model, args.dataset)
    model.set_L(args.L)
    model.set_T(args.time)
    model.to(device)
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print("\næ¨¡å‹ç»“æ„æ¦‚è§ˆ:")
    print("="*50)
    total_params = 0
    conv_params = 0
    fc_params = 0
    
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            num_params = module.weight.numel()
            total_params += num_params
            
            if isinstance(module, nn.Conv2d):
                conv_params += num_params
                layer_type = "Conv2d"
            else:
                fc_params += num_params
                layer_type = "Linear"
            
            print(f"{name} ({layer_type}): {module.weight.shape} - {num_params:,} å‚æ•°")
    
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"  å·ç§¯å±‚å‚æ•°: {conv_params:,}")
    print(f"  å…¨è¿æ¥å±‚å‚æ•°: {fc_params:,}")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print("="*50)
    
    # åˆ›å»ºHessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨
    print(f"\nåˆ›å»ºHessianæƒé‡é‡è¦æ€§è®¡ç®—å™¨ (é‡‡æ ·æ•°: {args.n_samples})...")
    hessian_calculator = HessianWeightImportance(
        model=model, 
        device=device,
        n_samples=args.n_samples
    )
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    print("\nå¼€å§‹Hessianæƒé‡é‡è¦æ€§åˆ†æ...")
    results = hessian_calculator.run_full_analysis(
        data_loader=train_loader,
        criterion=criterion
    )
    
    # è¯¦ç»†åˆ†æç»“æœ
    print("\nğŸ“Š è¯¦ç»†ç»“æœåˆ†æ:")
    print("="*80)
    
    # åˆ†æå±‚çº§é‡è¦æ€§
    layer_importance_summary = {}
    for name, importance_list in results['weight_importance'].items():
        layer_importance_summary[name] = {
            'mean_importance': sum(importance_list) / len(importance_list),
            'num_channels': len(importance_list),
            'total_importance': sum(importance_list)
        }
    
    # æŒ‰å¹³å‡é‡è¦æ€§æ’åº
    sorted_layers = sorted(layer_importance_summary.items(), 
                          key=lambda x: x[1]['mean_importance'], reverse=True)
    
    print("å„å±‚å¹³å‡é‡è¦æ€§æ’åº:")
    print(f"{'å±‚å':<30} {'å¹³å‡é‡è¦æ€§':<15} {'é€šé“æ•°':<10} {'æ€»é‡è¦æ€§':<15}")
    print("-" * 80)
    
    for name, stats in sorted_layers:
        print(f"{name:<30} {stats['mean_importance']:<15.6f} "
              f"{stats['num_channels']:<10} {stats['total_importance']:<15.6f}")
    
    # åˆ†æå·ç§¯å±‚vså…¨è¿æ¥å±‚
    conv_importances = []
    fc_importances = []
    
    for name, importance_list in results['weight_importance'].items():
        if 'layer' in name:  # å·ç§¯å±‚
            conv_importances.extend(importance_list)
        elif 'classifier' in name:  # å…¨è¿æ¥å±‚
            fc_importances.extend(importance_list)
    
    if conv_importances and fc_importances:
        print(f"\nğŸ“ˆ å±‚ç±»å‹å¯¹æ¯”:")
        print(f"å·ç§¯å±‚é‡è¦æ€§: å‡å€¼={sum(conv_importances)/len(conv_importances):.6f}, "
              f"é€šé“æ•°={len(conv_importances)}")
        print(f"å…¨è¿æ¥å±‚é‡è¦æ€§: å‡å€¼={sum(fc_importances)/len(fc_importances):.6f}, "
              f"ç¥ç»å…ƒæ•°={len(fc_importances)}")
    
    # å‰ªæå»ºè®®
    print(f"\nâœ‚ï¸ å‰ªæå»ºè®®:")
    pruning_candidates = results['pruning_candidates']
    print("å»ºè®®ä¼˜å…ˆå‰ªæçš„å±‚ï¼ˆé‡è¦æ€§æœ€ä½ï¼‰:")
    
    for name, channels in list(pruning_candidates.items())[:5]:  # æ˜¾ç¤ºå‰5å±‚
        avg_importance = sum([imp for _, imp in channels]) / len(channels)
        print(f"  {name}: {len(channels)} ä¸ªé€šé“, å¹³å‡é‡è¦æ€§={avg_importance:.6f}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒå‘ç°:")
    print(f"1. æˆåŠŸå®ç°äº† weight_importance = hessian_trace * (weight_norm^2 / num_weights)")
    print(f"2. é€šè¿‡Hutchinsonæ–¹æ³•é«˜æ•ˆä¼°è®¡äº†Hessian trace")
    print(f"3. å®ç°äº†channel-wiseçš„ç»†ç²’åº¦é‡è¦æ€§åˆ†æ")
    print(f"4. ä¸ºSNNå‰ªææä¾›äº†åŸºäºäºŒé˜¶ä¿¡æ¯çš„ç§‘å­¦ä¾æ®")
    
    return results


if __name__ == '__main__':
    main()
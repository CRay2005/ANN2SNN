#!/usr/bin/env python3
"""
é›†æˆæ”¹è¿›Hessiané‡è¦æ€§åˆ†æçš„è®­ç»ƒæµ‹è¯•
"""

import argparse
import torch
import torch.nn as nn
from Models import modelpool
from Preprocess import datapool
from improved_hessian_importance import ImprovedHessianWeightImportance


def main():
    print("ğŸ§ª é›†æˆæ”¹è¿›Hessiané‡è¦æ€§åˆ†æçš„è®­ç»ƒæµ‹è¯•")
    print("="*80)
    
    # å‚æ•°è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºVGG16æ¨¡å‹...")
    model = modelpool('vgg16', 'cifar10')
    model.set_L(8)
    model.set_T(0)  # ANNæ¨¡å¼åˆ†æ
    model.to(device)
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½CIFAR-10æ•°æ®...")
    train_loader, test_loader = datapool('cifar10', 32)
    
    # æ¨¡æ‹Ÿä¸€äº›è®­ç»ƒæ­¥éª¤
    print("\næ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx >= 5:  # åªè®­ç»ƒå‡ ä¸ªbatch
            break
        
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        print(f"  Batch {batch_idx+1}: Loss = {loss.item():.4f}")
    
    # åœ¨è®­ç»ƒååˆ†æIFå±‚é‡è¦æ€§
    print("\nğŸ” å¼€å§‹Hessiané‡è¦æ€§åˆ†æ...")
    hessian_calc = ImprovedHessianWeightImportance(
        model=model,
        device=device,
        n_samples=100,  # é€‚ä¸­çš„é‡‡æ ·æ•°ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
        use_double_precision=True
    )
    
    # è¿è¡Œåˆ†æ
    model.eval()
    results = hessian_calc.run_full_analysis(train_loader, criterion)
    
    # åº”ç”¨å‰ªæå»ºè®®
    print("\nğŸ¯ åº”ç”¨å‰ªæå»ºè®®:")
    print("="*50)
    
    pruning_candidates = results['pruning_candidates']
    print(f"å»ºè®®å‰ªæçš„IFå±‚ (å…±{len(pruning_candidates)}ä¸ª):")
    
    for layer_name, importance in pruning_candidates:
        print(f"  âœ‚ï¸  {layer_name}: é‡è¦æ€§ = {importance:.8f}")
    
    # å¯ä»¥åœ¨è¿™é‡Œå®é™…åº”ç”¨å‰ªæ
    # ä¾‹å¦‚ï¼Œè®¾ç½®æŸäº›IFå±‚çš„é˜ˆå€¼æˆ–ç¦ç”¨å®ƒä»¬
    
    print("\nğŸ“ˆ é‡è¦æ€§ç»Ÿè®¡:")
    valid_importances = results['valid_importances']
    if len(valid_importances) > 0:
        print(f"  æœ‰æ•ˆIFå±‚æ•°: {len(valid_importances)}")
        print(f"  å¹³å‡é‡è¦æ€§: {sum(valid_importances)/len(valid_importances):.8f}")
        print(f"  é‡è¦æ€§èŒƒå›´: [{min(valid_importances):.8f}, {max(valid_importances):.8f}]")
        
        # æŒ‰é‡è¦æ€§æ’åºæ˜¾ç¤º
        layer_importance_pairs = [(name, imp[0]) for name, imp in results['weight_importance'].items()]
        layer_importance_pairs.sort(key=lambda x: x[1], reverse=True)
        
        print("\n  ğŸ† é‡è¦æ€§æ’è¡Œæ¦œ (Top 5):")
        for i, (name, importance) in enumerate(layer_importance_pairs[:5]):
            print(f"    {i+1}. {name}: {importance:.8f}")
        
        print("\n  âš ï¸  æœ€ä¸é‡è¦ (Bottom 3):")
        for i, (name, importance) in enumerate(layer_importance_pairs[-3:]):
            print(f"    {len(layer_importance_pairs)-2+i}. {name}: {importance:.8f}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("="*80)
    
    return results


if __name__ == "__main__":
    results = main()
    
    # å¯ä»¥ä¿å­˜ç»“æœä¾›åç»­ä½¿ç”¨
    torch.save(results, 'hessian_importance_results.pt')
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° hessian_importance_results.pt") 
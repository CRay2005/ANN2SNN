"""
ANN-SNNæ¢¯åº¦åˆ†æå’Œç¥ç»å…ƒå‰ªæå·¥å…·
==================================

æœ¬æ–‡ä»¶å®ç°äº†ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š
1. GradientAnalyzer: åŸºäºæ¢¯åº¦å¹…åº¦åˆ†æçš„ç¥ç»å…ƒé‡è¦æ€§è¯„ä¼°å’Œå‰ªæ
2. SNNGradientSimulator: SNNæ¨¡æ‹Ÿæ¢¯åº¦åå‘ä¼ æ’­å·¥å…·ï¼Œç”¨äºSNNæ¨¡å¼ä¸‹çš„æ¢¯åº¦ä¼°è®¡

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒå…¨è¿æ¥å±‚çš„ç»†ç²’åº¦ç¥ç»å…ƒå‰ªæ
- æä¾›æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°ç”¨äºSNNåå‘ä¼ æ’­
- é›†æˆç»“æ„åŒ–å’Œéç»“æ„åŒ–å‰ªæç­–ç•¥
- æ”¯æŒæ¸è¿›å¼å‰ªæå’ŒåŠ¨æ€é˜ˆå€¼è°ƒæ•´

ä½œè€…ï¼šRay
æ—¥æœŸï¼š2024å¹´
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from collections import defaultdict
from typing import List, Tuple, Dict, Optional
from tqdm import tqdm

# ray æ–°å¢ - æ·»åŠ æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GradientAnalyzer:
    """
    åŸºäºæ¢¯åº¦åˆ†æçš„ç¥ç»å…ƒé‡è¦æ€§è¯„ä¼°å™¨
    
    è¯¥ç±»é€šè¿‡ç›‘æ§å…¨è¿æ¥å±‚ç¥ç»å…ƒçš„æ¢¯åº¦å˜åŒ–ï¼Œè¯†åˆ«é‡è¦æ€§è¾ƒä½çš„ç¥ç»å…ƒï¼Œ
    ä¸ºç¥ç»ç½‘ç»œå‰ªææä¾›ä¾æ®ã€‚
    """
    
    def __init__(self, model, prune_ratio=0.1, gradient_accumulation_steps=10):
        self.model = model
        self.prune_ratio = prune_ratio
        self.gradient_records = defaultdict(list)  # å­˜å‚¨å…¨è¿æ¥å±‚æ¢¯åº¦
        self.gradient_accumulation_steps = gradient_accumulation_steps  # ray æ–°å¢ - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        self.hooks = []  # ray æ–°å¢ - å­˜å‚¨é’©å­å¥æŸ„ï¼Œä¾¿äºæ¸…ç†
        
        # ray ä¿®æ”¹ - æ”¹è¿›é’©å­æ³¨å†Œé€»è¾‘ï¼Œå¢åŠ é”™è¯¯å¤„ç†
        self._register_hooks()
        
        logger.info(f"GradientAnalyzeråˆå§‹åŒ–å®Œæˆï¼Œå‰ªææ¯”ä¾‹: {prune_ratio}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    
    def _register_hooks(self):
        """ray æ–°å¢ - æ³¨å†Œæ¢¯åº¦é’©å­çš„ç§æœ‰æ–¹æ³•"""
        hook_count = 0
        # åªä¸ºå…¨è¿æ¥å±‚æ³¨å†Œæ¢¯åº¦é’©å­
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):  # ä»…å¤„ç†Linearå±‚
                # è·å–å…¨è¿æ¥å±‚çš„æƒé‡å‚æ•°
                weight_param = module.weight
                if weight_param.requires_grad:  # ray æ–°å¢ - æ£€æŸ¥å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦
                # ä¸ºæ¯ä¸ªå…¨è¿æ¥å±‚åˆ›å»ºå”¯ä¸€çš„æ ‡è¯†ç¬¦
                layer_id = f"fc_{name}"  # ä½¿ç”¨å±‚åä½œä¸ºå”¯ä¸€æ ‡è¯†
                
                # åˆ›å»ºé’©å­å‡½æ•°
                hook = self.make_hook(layer_id)
                    handle = weight_param.register_hook(hook)
                    self.hooks.append(handle)  # ray æ–°å¢ - ä¿å­˜é’©å­å¥æŸ„
                    hook_count += 1
                    
                    logger.info(f"ä¸ºå±‚ {layer_id} æ³¨å†Œæ¢¯åº¦é’©å­ï¼Œå‚æ•°å½¢çŠ¶: {weight_param.shape}")
        
        if hook_count == 0:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ³¨å†Œé’©å­çš„å…¨è¿æ¥å±‚ï¼")
        else:
            logger.info(f"æ€»å…±æ³¨å†Œäº† {hook_count} ä¸ªæ¢¯åº¦é’©å­")
    
    def make_hook(self, layer_id):
        def gradient_hook(grad):
            if grad is None:  # ray æ–°å¢ - æ¢¯åº¦ä¸ºç©ºçš„æ£€æŸ¥
                logger.warning(f"å±‚ {layer_id} çš„æ¢¯åº¦ä¸ºç©º")
                return
                
            # è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
            # æ¢¯åº¦å½¢çŠ¶: [out_features, in_features]
            # è®¡ç®—æ–¹å¼: å¯¹æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒæ±‚å…¶åœ¨è¾“å…¥ç‰¹å¾ä¸Šçš„æ¢¯åº¦å‡å€¼
            grad_mag = grad.abs().mean(dim=1)  # è¾“å‡ºå½¢çŠ¶: [out_features]
            self.gradient_records[layer_id].append(grad_mag.detach().cpu())
            
            # ray æ–°å¢ - é™åˆ¶å­˜å‚¨çš„æ¢¯åº¦è®°å½•æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
            if len(self.gradient_records[layer_id]) > self.gradient_accumulation_steps:
                self.gradient_records[layer_id].pop(0)
                
        return gradient_hook
    
    def clear_gradient_records(self):
        """ray æ–°å¢ - æ¸…ç©ºæ¢¯åº¦è®°å½•"""
        self.gradient_records.clear()
        logger.info("æ¢¯åº¦è®°å½•å·²æ¸…ç©º")
    
    def remove_hooks(self):
        """ray æ–°å¢ - ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hooks:
            handle.remove()
        self.hooks.clear()
        logger.info("æ‰€æœ‰æ¢¯åº¦é’©å­å·²ç§»é™¤")
    
    def get_gradient_statistics(self):
        """ray æ–°å¢ - è·å–æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for layer_id, grad_list in self.gradient_records.items():
            if grad_list:
                stacked_grads = torch.stack(grad_list)
                mean_grad = stacked_grads.mean(dim=0).mean(dim=0)  # æ—¶é—´å’Œæ‰¹æ¬¡ç»´åº¦çš„å¹³å‡
                stats[layer_id] = {
                    'mean': mean_grad.mean().item(),
                    'std': mean_grad.std().item(),
                    'min': mean_grad.min().item(),
                    'max': mean_grad.max().item(),
                    'num_records': len(grad_list)
                }
        return stats
    
    def get_low_grad_neurons(self):
        """è·å–ä½æ¢¯åº¦ç¥ç»å…ƒç”¨äºå‰ªæ"""
        all_neurons = []  # å­˜å‚¨è¦å‰ªæçš„ç¥ç»å…ƒä¿¡æ¯
        
        # ray æ–°å¢ - æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦è®°å½•
        if not self.gradient_records:
            logger.warning("æ²¡æœ‰æ¢¯åº¦è®°å½•ï¼Œæ— æ³•è¿›è¡Œç¥ç»å…ƒåˆ†æ")
            return []
        
        # éå†è®°å½•çš„æ‰€æœ‰å…¨è¿æ¥å±‚
        for layer_id, grad_list in self.gradient_records.items():
            if not grad_list:  # è·³è¿‡ç©ºè®°å½•
                logger.warning(f"å±‚ {layer_id} æ²¡æœ‰æ¢¯åº¦è®°å½•")
                continue
                
            # åˆå¹¶æ‰€æœ‰æ¢¯åº¦è®°å½•
            try:
                stacked_grads = torch.stack(grad_list)  # [timesteps, neurons] # ray ä¿®æ”¹ - ä¿®æ­£æ³¨é‡Š
            
                # è®¡ç®—æ—¶é—´ç»´åº¦çš„å¹³å‡æ¢¯åº¦ï¼ˆç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œå› ä¸ºæ¢¯åº¦é’©å­ä¸­å·²ç»å¤„ç†ï¼‰
            # ç»“æœä¸ºæ¯ä¸ªç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦å¹…åº¦
                neuron_avg_grad = stacked_grads.mean(dim=0)  # [neurons] # ray ä¿®æ”¹ - ç®€åŒ–è®¡ç®—
                
                logger.info(f"Layer: {layer_id}, Neurons: {len(neuron_avg_grad)}, Avg Grad: mean={neuron_avg_grad.mean():.6f}, std={neuron_avg_grad.std():.6f}")
                
            # æ”¶é›†è¯¥å±‚çš„ç¥ç»å…ƒæ¢¯åº¦ä¿¡æ¯
            for neuron_idx, grad_value in enumerate(neuron_avg_grad):
                # æ ¼å¼: (å±‚æ ‡è¯†ç¬¦, ç¥ç»å…ƒç´¢å¼•, æ¢¯åº¦å€¼)
                all_neurons.append((layer_id, neuron_idx, grad_value.item()))
                    
            except Exception as e:
                logger.error(f"å¤„ç†å±‚ {layer_id} çš„æ¢¯åº¦æ—¶å‡ºé”™: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰è®°å½•ä»»ä½•ç¥ç»å…ƒï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not all_neurons:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„ç¥ç»å…ƒ")
            return []
        
        # æŒ‰æ¢¯åº¦å€¼ä»å°åˆ°å¤§æ’åº
        all_neurons.sort(key=lambda x: x[2])
        
        # é€‰æ‹©æ¢¯åº¦æœ€å°çš„å‰ prune_ratio æ¯”ä¾‹çš„ç¥ç»å…ƒ
        num_to_select = int(len(all_neurons) * self.prune_ratio)
        selected_neurons = all_neurons[:num_to_select]
        
        logger.info(f"ä» {len(all_neurons)} ä¸ªç¥ç»å…ƒä¸­é€‰æ‹©äº† {len(selected_neurons)} ä¸ªä½æ¢¯åº¦ç¥ç»å…ƒè¿›è¡Œå‰ªæ")
        
        return selected_neurons

    def adaptive_prune_ratio(self, current_accuracy: float, target_accuracy: float = 0.9):
        """ray æ–°å¢ - è‡ªé€‚åº”å‰ªææ¯”ä¾‹è°ƒæ•´"""
        if current_accuracy > target_accuracy + 0.05:
            # ç²¾åº¦è¶³å¤Ÿé«˜ï¼Œå¯ä»¥å¢åŠ å‰ªææ¯”ä¾‹
            self.prune_ratio = min(self.prune_ratio * 1.2, 0.5)
        elif current_accuracy < target_accuracy - 0.02:
            # ç²¾åº¦å¤ªä½ï¼Œå‡å°‘å‰ªææ¯”ä¾‹
            self.prune_ratio = max(self.prune_ratio * 0.8, 0.01)
        
        logger.info(f"è‡ªé€‚åº”è°ƒæ•´å‰ªææ¯”ä¾‹ä¸º: {self.prune_ratio:.3f}")


class SNNGradientSimulator:
    """
    SNNæ¨¡æ‹Ÿæ¢¯åº¦åå‘ä¼ æ’­å·¥å…·
    
    è¯¥ç±»ä¸ºè„‰å†²ç¥ç»ç½‘ç»œæä¾›æ¨¡æ‹Ÿæ¢¯åº¦åŠŸèƒ½ï¼Œè§£å†³SNNè®­ç»ƒä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
    """
    
    def __init__(self, model, surrogate_fn=None, temperature=5.0):
        """
        SNNæ¨¡æ‹Ÿæ¢¯åº¦åå‘ä¼ æ’­å·¥å…·
        
        å‚æ•°:
        model -- SNNæ¨¡å‹
        surrogate_fn -- è‡ªå®šä¹‰æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°ï¼Œé»˜è®¤ä¸ºsigmoidæ¢¯åº¦è¿‘ä¼¼
        temperature -- æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°çš„æ¸©åº¦å‚æ•° # ray æ–°å¢
        """
        self.model = model
        self.gradient_records = defaultdict(list)
        self.activations = {}  # å­˜å‚¨å‰å‘æ¿€æ´»å€¼
        self.handles = []  # ray æ–°å¢ - å­˜å‚¨é’©å­å¥æŸ„
        self.temperature = temperature  # ray æ–°å¢
        
        # è®¾ç½®é»˜è®¤çš„æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°
        if surrogate_fn is not None:
            self.surrogate_grad = surrogate_fn
        else:
        self.surrogate_grad = self.default_surrogate_grad
        
        # æ³¨å†Œå‰å‘å’Œåå‘é’©å­
        self.register_hooks()
        
        logger.info(f"SNNGradientSimulatoråˆå§‹åŒ–å®Œæˆï¼Œæ¸©åº¦å‚æ•°: {temperature}")
    
    def default_surrogate_grad(self, x):
        """é»˜è®¤çš„æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•° - ä½¿ç”¨å¯è°ƒæ¸©åº¦çš„sigmoid"""
        # ray ä¿®æ”¹ - ä½¿ç”¨å¯è°ƒèŠ‚çš„æ¸©åº¦å‚æ•°
        sg = torch.sigmoid(self.temperature * x)
        return sg * (1 - sg)
    
    def triangular_surrogate_grad(self, x):
        """ray æ–°å¢ - ä¸‰è§’å½¢æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°"""
        return torch.clamp(1.0 - torch.abs(x), 0.0, 1.0)
    
    def rectangular_surrogate_grad(self, x):
        """ray æ–°å¢ - çŸ©å½¢æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°"""
        return (torch.abs(x) <= 0.5).float()
    
    def register_hooks(self):
        """ä¸ºå…¨è¿æ¥å±‚æ³¨å†Œé’©å­"""
        # æ¸…é™¤æ‰€æœ‰ç°æœ‰é’©å­
        self.remove_hooks()  # ray ä¿®æ”¹ - ä½¿ç”¨ä¸“é—¨çš„æ¸…ç†æ–¹æ³•
        
        hook_count = 0
        # æ³¨å†Œå‰å‘é’©å­ä»¥å­˜å‚¨æ¿€æ´»å€¼
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):  # ä»…å¤„ç†Linearå±‚
                # å‰å‘é’©å­å­˜å‚¨æ¿€æ´»å€¼
                forward_hook = self.make_forward_hook(name)
                handle = module.register_forward_hook(forward_hook)
                self.handles.append(handle)
                
                # åå‘é’©å­åº”ç”¨æ¨¡æ‹Ÿæ¢¯åº¦
                backward_hook = self.make_backward_hook(name)
                handle = module.register_full_backward_hook(backward_hook)
                self.handles.append(handle)
                
                hook_count += 1
                logger.info(f"ä¸ºå±‚ {name} æ³¨å†Œå‰å‘å’Œåå‘é’©å­")
        
        logger.info(f"æ€»å…±æ³¨å†Œäº† {hook_count * 2} ä¸ªé’©å­ï¼ˆå‰å‘+åå‘ï¼‰")
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.handles:
            handle.remove()
        self.handles = []
        logger.info("æ‰€æœ‰é’©å­å·²ç§»é™¤")
    
    def make_forward_hook(self, name):
        """åˆ›å»ºå‰å‘é’©å­"""
        def forward_hook(module, input, output):
            # ray ä¿®å¤ - æ›´å®‰å…¨çš„å‰å‘é’©å­å®ç°
            try:
                if output is not None:
            # å­˜å‚¨å½“å‰æ¨¡å—çš„æ¿€æ´»å€¼ï¼ˆç”¨äºæ¨¡æ‹Ÿæ¢¯åº¦è®¡ç®—ï¼‰
            self.activations[name] = output.detach().clone()
                    logger.debug(f"å±‚ {name} æ¿€æ´»å€¼å·²ä¿å­˜: {output.shape}")
            except Exception as e:
                logger.warning(f"ä¿å­˜å±‚ {name} æ¿€æ´»å€¼æ—¶å‡ºé”™: {e}")
        return forward_hook
    
    def make_backward_hook(self, name):
        """åˆ›å»ºåå‘é’©å­ï¼ˆåº”ç”¨æ¨¡æ‹Ÿæ¢¯åº¦ï¼‰"""
        def backward_hook(module, grad_input, grad_output):
            # ray ä¿®å¤ - æ›´å®‰å…¨çš„åå‘é’©å­å®ç°ï¼Œé¿å…æ”¹å˜æ¢¯åº¦å¤§å°
            if grad_output is None or len(grad_output) == 0 or grad_output[0] is None:
                return None
            
            # è·å–æ¨¡å—æ¿€æ´»å€¼ï¼ˆå¦‚æœæ²¡æœ‰è®°å½•ï¼Œåˆ™è¿”å›åŸæ¢¯åº¦ï¼‰
            if name not in self.activations:
                logger.debug(f"å±‚ {name} æ²¡æœ‰è®°å½•æ¿€æ´»å€¼ï¼Œè·³è¿‡æ¨¡æ‹Ÿæ¢¯åº¦")
                return None  # è¿”å›Noneè¡¨ç¤ºä¸ä¿®æ”¹æ¢¯åº¦
            
            try:
            # è®¡ç®—æ¨¡æ‹Ÿæ¢¯åº¦
            activations = self.activations[name]
            surrogate = self.surrogate_grad(activations)
            
                # ray ä¿®å¤ - åªåœ¨å½¢çŠ¶å®Œå…¨åŒ¹é…æ—¶æ‰åº”ç”¨æ¨¡æ‹Ÿæ¢¯åº¦
            modified_grad_output = []
                for i, grad_out in enumerate(grad_output):
                if grad_out is not None:
                        # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦å®Œå…¨åŒ¹é…
                    if grad_out.shape == surrogate.shape:
                            # å½¢çŠ¶åŒ¹é…ï¼Œå®‰å…¨åº”ç”¨æ¨¡æ‹Ÿæ¢¯åº¦
                        modified_grad_out = grad_out * surrogate
                    else:
                            # ray ä¿®å¤ - å½¢çŠ¶ä¸åŒ¹é…æ—¶ä¸ä¿®æ”¹æ¢¯åº¦ï¼Œé¿å…å¤§å°æ”¹å˜
                            logger.debug(f"å±‚ {name} å½¢çŠ¶ä¸åŒ¹é…: grad{grad_out.shape} vs surrogate{surrogate.shape}ï¼Œä¿æŒåŸæ¢¯åº¦")
                            modified_grad_out = grad_out
                        
                        # ray ä¿®å¤ - ç¡®ä¿è¾“å‡ºæ¢¯åº¦ä¸è¾“å…¥æ¢¯åº¦å½¢çŠ¶å®Œå…¨ä¸€è‡´
                        assert modified_grad_out.shape == grad_out.shape, f"æ¢¯åº¦å½¢çŠ¶å‘ç”Ÿå˜åŒ–: {grad_out.shape} -> {modified_grad_out.shape}"
                    modified_grad_output.append(modified_grad_out)
                else:
                    modified_grad_output.append(None)
            
                # ray ä¿®å¤ - ç¡®ä¿è¿”å›çš„tupleé•¿åº¦ä¸è¾“å…¥ä¸€è‡´
                assert len(modified_grad_output) == len(grad_output), "ä¿®æ”¹åçš„æ¢¯åº¦è¾“å‡ºé•¿åº¦ä¸åŒ¹é…"
                
            # è¿”å›ä¿®æ”¹åçš„æ¢¯åº¦
            return tuple(modified_grad_output)
                
            except Exception as e:
                logger.warning(f"åœ¨å±‚ {name} åº”ç”¨æ¨¡æ‹Ÿæ¢¯åº¦æ—¶å‡ºé”™: {e}ï¼Œè¿”å›åŸæ¢¯åº¦")
                return None  # å‡ºé”™æ—¶è¿”å›Noneï¼Œä¿æŒåŸæ¢¯åº¦
        
        return backward_hook
    
    def analyze_fc_gradients(self, prune_ratio=0.1):
        """
        åˆ†æå…¨è¿æ¥å±‚æ¢¯åº¦å¹¶è¿”å›ä½æ¢¯åº¦ç¥ç»å…ƒ
        å¿…é¡»å…ˆè¿è¡Œåå‘ä¼ æ’­æ‰èƒ½è°ƒç”¨æ­¤æ–¹æ³•
        
        å‚æ•°:
        prune_ratio -- å‰ªææ¯”ä¾‹ï¼Œé»˜è®¤0.1
        
        è¿”å›:
        ä½æ¢¯åº¦ç¥ç»å…ƒåˆ—è¡¨ (å±‚å, ç¥ç»å…ƒç´¢å¼•, æ¢¯åº¦å€¼)
        """
        # æ”¶é›†å…¨è¿æ¥å±‚çš„æ¢¯åº¦ä¿¡æ¯
        fc_gradients = {}
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear) and module.weight.grad is not None:
                # è·å–æƒé‡æ¢¯åº¦
                weight_grad = module.weight.grad.detach()
                
                # è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å¹³å‡æ¢¯åº¦
                # å¯¹äºå…¨è¿æ¥å±‚ï¼Œå¯¹è¾“å…¥ç»´åº¦å–å¹³å‡
                neuron_grads = weight_grad.abs().mean(dim=1)
                
                fc_gradients[name] = {
                    'gradients': neuron_grads,
                    'module': module
                }
                
                logger.info(f"åˆ†æå±‚ {name}: {module.out_features} ä¸ªç¥ç»å…ƒ")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…¨è¿æ¥å±‚æ¢¯åº¦ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not fc_gradients:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å…¨è¿æ¥å±‚æ¢¯åº¦")
            return []
        
        # èšåˆæ‰€æœ‰ç¥ç»å…ƒæ¢¯åº¦
        all_neurons = []
        for layer_name, data in fc_gradients.items():
            grads = data['gradients']
            module = data['module']
            
            for neuron_idx in range(module.out_features):
                grad_value = grads[neuron_idx].item()
                all_neurons.append((layer_name, neuron_idx, grad_value))
        
        # æŒ‰æ¢¯åº¦å€¼æ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰
        all_neurons.sort(key=lambda x: x[2])
        
        # é€‰æ‹©æ¢¯åº¦æœ€å°çš„ç¥ç»å…ƒ
        num_prune = int(len(all_neurons) * prune_ratio)
        selected_neurons = all_neurons[:num_prune]
        
        logger.info(f"ä» {len(all_neurons)} ä¸ªç¥ç»å…ƒä¸­é€‰æ‹© {len(selected_neurons)} ä¸ªè¿›è¡Œå‰ªæ")
        
        return selected_neurons
    
    def prune_neurons(self, neurons_to_prune):
        """å‰ªæä½æ¢¯åº¦ç¥ç»å…ƒ"""
        pruned_count = 0
        
        for layer_name, neuron_idx, grad_value in neurons_to_prune:
            # æ‰¾åˆ°å¯¹åº”æ¨¡å—
            module = None
            for name, mod in self.model.named_modules():
                if name == layer_name and isinstance(mod, nn.Linear):
                    module = mod
                    break
            
            if module is None:
                logger.warning(f"æœªæ‰¾åˆ°å±‚ {layer_name}")
                continue
                
            # ray æ–°å¢ - æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
            if neuron_idx >= module.out_features:
                logger.warning(f"ç¥ç»å…ƒç´¢å¼• {neuron_idx} è¶…å‡ºå±‚ {layer_name} çš„èŒƒå›´ ({module.out_features})")
                continue
                
            # æ‰§è¡Œå‰ªæï¼ˆå°†å¯¹åº”ç¥ç»å…ƒçš„æƒé‡ç½®é›¶ï¼‰
            with torch.no_grad():
                # å‰ªæè¾“å‡ºæƒé‡
                module.weight.data[neuron_idx] = 0
                
                # å¦‚æœæœ‰åç½®é¡¹ï¼Œå‰ªæåç½®
                if module.bias is not None:
                    module.bias.data[neuron_idx] = 0
                
                # ray ä¿®æ”¹ - æ”¹è¿›ä¸‹æ¸¸è¿æ¥å‰ªæ
                self.prune_downstream_connections(layer_name, neuron_idx)
                
                pruned_count += 1
                logger.debug(f"å‰ªæå±‚ {layer_name} ç¥ç»å…ƒ {neuron_idx} (æ¢¯åº¦å€¼: {grad_value:.6f})")
        
        logger.info(f"æˆåŠŸå‰ªæäº† {pruned_count} ä¸ªç¥ç»å…ƒ")
    
    def prune_downstream_connections(self, pruned_layer_name: str, pruned_neuron_idx: int):
        """ray ä¿®æ”¹ - æ”¹è¿›ä¸‹æ¸¸è¿æ¥å‰ªæé€»è¾‘"""
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½å—å½±å“çš„ä¸‹æ¸¸å±‚
        layer_names = list(dict(self.model.named_modules()).keys())
        
        try:
            current_idx = layer_names.index(pruned_layer_name)
        except ValueError:
            logger.warning(f"æ— æ³•æ‰¾åˆ°å±‚ {pruned_layer_name} çš„ä½ç½®ç´¢å¼•")
            return
        
        # æŸ¥æ‰¾åç»­çš„Linearå±‚
        for i in range(current_idx + 1, len(layer_names)):
            layer_name = layer_names[i]
            module = dict(self.model.named_modules())[layer_name]
            
            if isinstance(module, nn.Linear):
                # æ£€æŸ¥è¾“å…¥ç»´åº¦æ˜¯å¦åŒ¹é…
                if pruned_neuron_idx < module.in_features:
                with torch.no_grad():
                        # å‰ªæå¯¹åº”çš„è¾“å…¥è¿æ¥
                    module.weight.data[:, pruned_neuron_idx] = 0
                        logger.debug(f"å‰ªæä¸‹æ¸¸å±‚ {layer_name} çš„è¾“å…¥è¿æ¥ {pruned_neuron_idx}")
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªä¸‹æ¸¸Linearå±‚
    
    def find_downstream_modules(self, target_module):
        """ray ä¿®æ”¹ - æ”¹è¿›ä¸‹æ¸¸æ¨¡å—æŸ¥æ‰¾é€»è¾‘"""
        downstream = []
        modules_list = list(self.model.modules())
        
        try:
            target_idx = modules_list.index(target_module)
            # æŸ¥æ‰¾ç›´æ¥çš„ä¸‹æ¸¸Linearå±‚
            for i in range(target_idx + 1, len(modules_list)):
                module = modules_list[i]
                if isinstance(module, nn.Linear):
                downstream.append(module)
                    break  # åªå–ç¬¬ä¸€ä¸ªä¸‹æ¸¸Linearå±‚
        except ValueError:
            logger.warning("æ— æ³•æ‰¾åˆ°ç›®æ ‡æ¨¡å—åœ¨æ¨¡å‹ä¸­çš„ä½ç½®")
        
        return downstream
    
    def get_activation_statistics(self):
        """ray æ–°å¢ - è·å–æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for name, activation in self.activations.items():
            if activation is not None:
                stats[name] = {
                    'mean': activation.mean().item(),
                    'std': activation.std().item(),
                    'min': activation.min().item(),
                    'max': activation.max().item(),
                    'shape': list(activation.shape),
                    'sparsity': (activation == 0).float().mean().item()
                }
        return stats


# ray æ–°å¢ - ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å…·å‡½æ•°
def create_sample_model():
    """åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ¨¡å‹ç”¨äºæµ‹è¯•"""
    model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
    return model

def demo_gradient_analysis():
    """ray æ–°å¢ - æ¢¯åº¦åˆ†ææ¼”ç¤ºå‡½æ•°"""
    logger.info("å¼€å§‹æ¢¯åº¦åˆ†ææ¼”ç¤º...")
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹å’Œæ•°æ®
    model = create_sample_model()
    dummy_input = torch.randn(32, 784)
    dummy_target = torch.randint(0, 10, (32,))
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = GradientAnalyzer(model, prune_ratio=0.2)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    
    for epoch in range(5):
        model.train()
        optimizer.zero_grad()
        
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        loss.backward()
        
        optimizer.step()
        
        if epoch % 2 == 0:
            logger.info(f"Epoch {epoch}, Loss: {loss.item():.4f}")
    
    # åˆ†æä½æ¢¯åº¦ç¥ç»å…ƒ
low_grad_neurons = analyzer.get_low_grad_neurons()
    logger.info(f"å‘ç° {len(low_grad_neurons)} ä¸ªä½æ¢¯åº¦ç¥ç»å…ƒ")
    
    # è·å–æ¢¯åº¦ç»Ÿè®¡
    stats = analyzer.get_gradient_statistics()
    for layer_id, stat in stats.items():
        logger.info(f"Layer {layer_id}: mean={stat['mean']:.6f}, std={stat['std']:.6f}")
    
    # æ¸…ç†
    analyzer.remove_hooks()
    logger.info("æ¼”ç¤ºå®Œæˆ")

def test_snn_gradient_simulator():
    """ray æ–°å¢ - SNNGradientSimulatorçš„è¯¦ç»†æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹SNNGradientSimulatorå®Œæ•´æµ‹è¯•")
    logger.info("=" * 80)
    
    # 1. åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestSNNModel(nn.Module):
        def __init__(self):
            super(TestSNNModel, self).__init__()
            self.fc1 = nn.Linear(784, 256)
            self.relu1 = nn.ReLU()
            self.fc2 = nn.Linear(256, 128) 
            self.relu2 = nn.ReLU()
            self.fc3 = nn.Linear(128, 10)
            
        def forward(self, x):
            x = self.fc1(x)
            x = self.relu1(x)
            x = self.fc2(x)
            x = self.relu2(x)
            x = self.fc3(x)
            return x
    
    model = TestSNNModel()
    logger.info("âœ… æµ‹è¯•æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # 2. æµ‹è¯•SNNGradientSimulatoråˆå§‹åŒ–
    logger.info("\nğŸ“‹ æµ‹è¯•1: åˆå§‹åŒ–å’ŒåŸºæœ¬åŠŸèƒ½")
    simulator = SNNGradientSimulator(model, temperature=5.0)
    logger.info("âœ… SNNGradientSimulatoråˆå§‹åŒ–æˆåŠŸ")
    
    # 3. å‡†å¤‡æµ‹è¯•æ•°æ®
    batch_size = 32
    input_data = torch.randn(batch_size, 784)
    target_data = torch.randint(0, 10, (batch_size,))
    logger.info(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ: input{input_data.shape}, target{target_data.shape}")
    
    # 4. æµ‹è¯•å‰å‘ä¼ æ’­å’Œé’©å­åŠŸèƒ½
    logger.info("\nğŸ“‹ æµ‹è¯•2: å‰å‘ä¼ æ’­å’Œé’©å­åŠŸèƒ½")
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
    for step in range(3):
        optimizer.zero_grad()
        output = model(input_data)
        loss = criterion(output, target_data)
        loss.backward()
        optimizer.step()
        logger.info(f"  Step {step+1}: Loss = {loss.item():.4f}")
    
    logger.info("âœ… å‰å‘ä¼ æ’­å’Œé’©å­æµ‹è¯•å®Œæˆ")
    
    # 5. æµ‹è¯•æ¿€æ´»ç»Ÿè®¡åŠŸèƒ½
    logger.info("\nğŸ“‹ æµ‹è¯•3: æ¿€æ´»ç»Ÿè®¡åŠŸèƒ½")
    activation_stats = simulator.get_activation_statistics()
    
    if activation_stats:
        logger.info("æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯:")
        for layer_name, stats in activation_stats.items():
            logger.info(f"  {layer_name}:")
            logger.info(f"    å‡å€¼: {stats['mean']:.4f}, æ ‡å‡†å·®: {stats['std']:.4f}")  
            logger.info(f"    ç¨€ç–åº¦: {stats['sparsity']:.4f}, å½¢çŠ¶: {stats['shape']}")
    else:
        logger.warning("âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯")
    
    # 6. æµ‹è¯•æ¢¯åº¦åˆ†æåŠŸèƒ½
    logger.info("\nğŸ“‹ æµ‹è¯•4: æ¢¯åº¦åˆ†æåŠŸèƒ½")
    
    # é‡æ–°è¿›è¡Œå‰å‘ä¼ æ’­ä»¥è·å–å½“å‰æ¢¯åº¦
    optimizer.zero_grad()
    output = model(input_data)
    loss = criterion(output, target_data)
    loss.backward()
    
    # åˆ†æå…¨è¿æ¥å±‚æ¢¯åº¦
    low_grad_neurons = simulator.analyze_fc_gradients(prune_ratio=0.2)
    logger.info(f"âœ… å‘ç° {len(low_grad_neurons)} ä¸ªä½æ¢¯åº¦ç¥ç»å…ƒå¾…å‰ªæ")
    
    if low_grad_neurons:
        logger.info("å‰5ä¸ªä½æ¢¯åº¦ç¥ç»å…ƒ:")
        for i, (layer_name, neuron_idx, grad_value) in enumerate(low_grad_neurons[:5]):
            logger.info(f"  {i+1}. å±‚: {layer_name}, ç¥ç»å…ƒ: {neuron_idx}, æ¢¯åº¦: {grad_value:.6f}")
    
    # 7. æµ‹è¯•ä¸åŒæ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°
    logger.info("\nğŸ“‹ æµ‹è¯•5: ä¸åŒæ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°")
    
    # æµ‹è¯•è¾“å…¥
    test_x = torch.tensor([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5])
    
    # Sigmoidæ¨¡æ‹Ÿæ¢¯åº¦
    sigmoid_grad = simulator.default_surrogate_grad(test_x)
    logger.info(f"Sigmoidæ¢¯åº¦: {[f'{x:.3f}' for x in sigmoid_grad.tolist()]}")
    
    # ä¸‰è§’å½¢æ¨¡æ‹Ÿæ¢¯åº¦
    triangular_grad = simulator.triangular_surrogate_grad(test_x)
    logger.info(f"ä¸‰è§’å½¢æ¢¯åº¦: {[f'{x:.3f}' for x in triangular_grad.tolist()]}")
    
    # çŸ©å½¢æ¨¡æ‹Ÿæ¢¯åº¦
    rectangular_grad = simulator.rectangular_surrogate_grad(test_x)
    logger.info(f"çŸ©å½¢æ¢¯åº¦: {[f'{x:.3f}' for x in rectangular_grad.tolist()]}")
    
    # 8. æµ‹è¯•å‰ªæåŠŸèƒ½
    logger.info("\nğŸ“‹ æµ‹è¯•6: ç¥ç»å…ƒå‰ªæåŠŸèƒ½")
    
    if low_grad_neurons:
        # è®°å½•å‰ªæå‰çš„å‚æ•°çŠ¶æ€
        logger.info("å‰ªæå‰æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        for name, param in model.named_parameters():
            if 'weight' in name:
                zero_count = (param == 0).sum().item()
                total_count = param.numel()
                logger.info(f"  {name}: é›¶å…ƒç´  {zero_count}/{total_count}")
        
        # æ‰§è¡Œå‰ªæï¼ˆåªé€‰æ‹©å‰3ä¸ªè¿›è¡Œæµ‹è¯•ï¼‰
        test_prune_neurons = low_grad_neurons[:3]
        logger.info(f"å¼€å§‹å‰ªæ {len(test_prune_neurons)} ä¸ªç¥ç»å…ƒ...")
        simulator.prune_neurons(test_prune_neurons)
        
        # æ£€æŸ¥å‰ªææ•ˆæœ
        logger.info("å‰ªæåæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        for name, param in model.named_parameters():
            if 'weight' in name:
                zero_count = (param == 0).sum().item()
                total_count = param.numel()
                logger.info(f"  {name}: é›¶å…ƒç´  {zero_count}/{total_count}")
        
        logger.info("âœ… å‰ªæåŠŸèƒ½æµ‹è¯•å®Œæˆ")
    else:
        logger.info("âš ï¸ æ²¡æœ‰ä½æ¢¯åº¦ç¥ç»å…ƒï¼Œè·³è¿‡å‰ªææµ‹è¯•")
    
    # 9. æµ‹è¯•æ¸©åº¦å‚æ•°å½±å“
    logger.info("\nğŸ“‹ æµ‹è¯•7: æ¸©åº¦å‚æ•°å½±å“")
    
    temperatures = [1.0, 3.0, 5.0, 10.0]
    test_input = torch.tensor([1.0])
    
    for temp in temperatures:
        temp_simulator = SNNGradientSimulator(model, temperature=temp)
        grad_val = temp_simulator.default_surrogate_grad(test_input)
        logger.info(f"  æ¸©åº¦T={temp}: æ¢¯åº¦å€¼={grad_val.item():.4f}")
        temp_simulator.remove_hooks()
    
    # 10. æµ‹è¯•é”™è¯¯å¤„ç†
    logger.info("\nğŸ“‹ æµ‹è¯•8: é”™è¯¯å¤„ç†")
    
    # æµ‹è¯•ç©ºæ¢¯åº¦å¤„ç†
    empty_model = nn.Linear(10, 5)
    empty_simulator = SNNGradientSimulator(empty_model)
    empty_result = empty_simulator.analyze_fc_gradients()
    logger.info(f"ç©ºæ¢¯åº¦å¤„ç†: è¿”å› {len(empty_result)} ä¸ªç¥ç»å…ƒ")
    empty_simulator.remove_hooks()
    
    # 11. æ€§èƒ½æµ‹è¯•
    logger.info("\nğŸ“‹ æµ‹è¯•9: æ€§èƒ½æµ‹è¯•")
    
    # å¤§æ‰¹é‡æ•°æ®æµ‹è¯•
    large_input = torch.randn(128, 784)
    large_target = torch.randint(0, 10, (128,))
    
    import time
    start_time = time.time()
    
    optimizer.zero_grad()
    output = model(large_input)
    loss = criterion(output, large_target)
    loss.backward()
    
    end_time = time.time()
    logger.info(f"å¤§æ‰¹é‡æ•°æ®å¤„ç†æ—¶é—´: {end_time - start_time:.4f}ç§’")
    
    # 12. æ¸…ç†èµ„æº
    logger.info("\nğŸ“‹ æµ‹è¯•10: èµ„æºæ¸…ç†")
    simulator.remove_hooks()
    logger.info("âœ… æ‰€æœ‰é’©å­å·²æ¸…ç†")
    
    # éªŒè¯é’©å­ç¡®å®è¢«æ¸…ç†
    hook_count = len(simulator.handles)
    logger.info(f"æ¸…ç†åé’©å­æ•°é‡: {hook_count}")
    
    # æµ‹è¯•æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ SNNGradientSimulatoræµ‹è¯•å®Œæˆ!")
    logger.info("=" * 80)
    
    logger.info("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    logger.info("âœ… é”™è¯¯å¤„ç†æœºåˆ¶æœ‰æ•ˆ")
    logger.info("âœ… èµ„æºç®¡ç†æ­£ç¡®")
    logger.info("âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½")
    
    logger.info("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    logger.info("1. æ¨èæ¸©åº¦å‚æ•°T=5.0ç”¨äºå¤§å¤šæ•°åº”ç”¨")
    logger.info("2. å‰ªææ¯”ä¾‹ä»10-20%å¼€å§‹ï¼Œæ ¹æ®æ€§èƒ½è°ƒæ•´")
    logger.info("3. å®šæœŸè°ƒç”¨get_activation_statistics()ç›‘æµ‹æ¨¡å‹çŠ¶æ€")
    logger.info("4. ä½¿ç”¨å®Œæ¯•ååŠ¡å¿…è°ƒç”¨remove_hooks()æ¸…ç†èµ„æº")
    
    return simulator, model

# ray æ–°å¢ - å¿«é€Ÿæµ‹è¯•å‡½æ•°
def quick_test_snn_simulator():
    """å¿«é€Ÿæµ‹è¯•SNNGradientSimulatorçš„ä¸»è¦åŠŸèƒ½"""
    logger.info("ğŸš€ å¿«é€Ÿæµ‹è¯•SNNGradientSimulator")
    
    try:
        # åˆ›å»ºç®€å•æ¨¡å‹
        model = nn.Sequential(
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50, 10)
        )
        
        # åˆ›å»ºæ¨¡æ‹Ÿå™¨
        simulator = SNNGradientSimulator(model, temperature=5.0)
        
        # ç®€å•è®­ç»ƒ
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        criterion = nn.CrossEntropyLoss()
        
        input_data = torch.randn(16, 100)
        target_data = torch.randint(0, 10, (16,))
        
        model.train()
        optimizer.zero_grad()
        output = model(input_data)
        loss = criterion(output, target_data)
        
        logger.info(f"å¿«é€Ÿæµ‹è¯• - æŸå¤±: {loss.item():.4f}")
        
        # ray ä¿®å¤ - å®‰å…¨çš„åå‘ä¼ æ’­
        try:
            loss.backward()
            optimizer.step()
            logger.info("âœ… åå‘ä¼ æ’­æˆåŠŸ")
        except Exception as e:
            logger.warning(f"åå‘ä¼ æ’­å‡ºé”™: {e}ï¼Œä½†æµ‹è¯•ç»§ç»­")
        
        # æ¢¯åº¦åˆ†æ
        try:
            low_grad_neurons = simulator.analyze_fc_gradients(prune_ratio=0.3)
            logger.info(f"å‘ç° {len(low_grad_neurons)} ä¸ªä½æ¢¯åº¦ç¥ç»å…ƒ")
        except Exception as e:
            logger.warning(f"æ¢¯åº¦åˆ†æå‡ºé”™: {e}")
            low_grad_neurons = []
        
        # è·å–ç»Ÿè®¡
        try:
            stats = simulator.get_activation_statistics()
            logger.info(f"æ”¶é›†åˆ° {len(stats)} å±‚çš„æ¿€æ´»ç»Ÿè®¡")
        except Exception as e:
            logger.warning(f"è·å–æ¿€æ´»ç»Ÿè®¡å‡ºé”™: {e}")
            stats = {}
        
        # æ¸…ç†
        simulator.remove_hooks()
        logger.info("âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ")
        
        return len(low_grad_neurons) >= 0  # è¿”å›æµ‹è¯•æ˜¯å¦æˆåŠŸ
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

# ray æ–°å¢ - åŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼ˆä¸ä½¿ç”¨æ¨¡æ‹Ÿæ¢¯åº¦é’©å­ï¼‰
def basic_snn_test():
    """åŸºç¡€SNNGradientSimulatoræµ‹è¯•ï¼Œä¸ä½¿ç”¨åå‘é’©å­é¿å…é”™è¯¯"""
    logger.info("ğŸ”§ åŸºç¡€SNNGradientSimulatoråŠŸèƒ½æµ‹è¯•")
    
    try:
        # åˆ›å»ºç®€å•æ¨¡å‹
        model = nn.Linear(10, 5)
        
        # åˆ›å»ºSNNGradientSimulatorï¼Œä½†ä¸æ³¨å†Œé’©å­
        simulator = SNNGradientSimulator.__new__(SNNGradientSimulator)
        simulator.model = model
        simulator.gradient_records = defaultdict(list)
        simulator.activations = {}
        simulator.handles = []
        simulator.temperature = 5.0
        simulator.surrogate_grad = lambda x: torch.sigmoid(5.0 * x) * (1 - torch.sigmoid(5.0 * x))
        
        logger.info("âœ… SNNGradientSimulatoråˆ›å»ºæˆåŠŸï¼ˆæ— é’©å­ï¼‰")
        
        # æµ‹è¯•æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°
        test_input = torch.tensor([-1.0, 0.0, 1.0])
        sigmoid_grad = simulator.surrogate_grad(test_input)
        logger.info(f"Sigmoidæ¢¯åº¦æµ‹è¯•: {sigmoid_grad.tolist()}")
        
        # æµ‹è¯•å…¶ä»–æ¨¡æ‹Ÿæ¢¯åº¦å‡½æ•°
        triangular_grad = torch.clamp(1.0 - torch.abs(test_input), 0.0, 1.0)
        rectangular_grad = (torch.abs(test_input) <= 0.5).float()
        
        logger.info(f"ä¸‰è§’å½¢æ¢¯åº¦: {triangular_grad.tolist()}")
        logger.info(f"çŸ©å½¢æ¢¯åº¦: {rectangular_grad.tolist()}")
        
        # æ¨¡æ‹Ÿä¸€äº›æ¿€æ´»æ•°æ®
        simulator.activations['test_layer'] = torch.randn(16, 5)
        
        # æµ‹è¯•æ¿€æ´»ç»Ÿè®¡
        stats = {}
        for name, activation in simulator.activations.items():
            stats[name] = {
                'mean': activation.mean().item(),
                'std': activation.std().item(),
                'sparsity': (activation == 0).float().mean().item(),
                'shape': list(activation.shape)
            }
        
        logger.info(f"æ¿€æ´»ç»Ÿè®¡æµ‹è¯•: {stats}")
        
        logger.info("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"åŸºç¡€æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # ray æ–°å¢ - ä¸»ç¨‹åºå…¥å£æ›´æ–°
    logger.info("æ¢¯åº¦åˆ†æå·¥å…·æµ‹è¯•")
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "snn":
        # å®Œæ•´çš„SNNæµ‹è¯•
        try:
            test_snn_gradient_simulator()
        except Exception as e:
            logger.error(f"å®Œæ•´SNNæµ‹è¯•å¤±è´¥: {e}")
            logger.info("å°è¯•è¿è¡ŒåŸºç¡€æµ‹è¯•...")
            basic_snn_test()
    elif len(sys.argv) > 1 and sys.argv[1] == "quick":
        # å¿«é€Ÿæµ‹è¯•
        success = quick_test_snn_simulator()
        if success:
            logger.info("âœ… å¿«é€Ÿæµ‹è¯•é€šè¿‡")
        else:
            logger.error("âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥")
    elif len(sys.argv) > 1 and sys.argv[1] == "basic":
        # ray æ–°å¢ - åŸºç¡€æµ‹è¯•æ¨¡å¼
        success = basic_snn_test()
        if success:
            logger.info("âœ… åŸºç¡€æµ‹è¯•é€šè¿‡")
        else:
            logger.error("âŒ åŸºç¡€æµ‹è¯•å¤±è´¥")
    else:
        # é»˜è®¤æ¼”ç¤º
        try:
            demo_gradient_analysis()
            logger.info("\nğŸ’¡ ä½¿ç”¨æç¤º:")
            logger.info("- è¿è¡Œ 'python 00raytest.py snn' è¿›è¡Œå®Œæ•´SNNæµ‹è¯•")
            logger.info("- è¿è¡Œ 'python 00raytest.py quick' è¿›è¡Œå¿«é€Ÿæµ‹è¯•")
            logger.info("- è¿è¡Œ 'python 00raytest.py basic' è¿›è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•")
        except Exception as e:
            logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")